{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "154f02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# --- 1. Environment ---\n",
    "class TicTacToeEnv:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros(9, dtype=np.int8)\n",
    "        self.agent = 1\n",
    "        self.opponent = -1\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board[:] = 0\n",
    "        self.mover = np.random.choice([1, -1])\n",
    "        return self.board.copy()\n",
    "\n",
    "    def available_actions(self):\n",
    "        return np.where(self.board == 0)[0]\n",
    "\n",
    "    def reward_done(self):\n",
    "        lines = [\n",
    "            (0,1,2),(3,4,5),(6,7,8),\n",
    "            (0,3,6),(1,4,7),(2,5,8),\n",
    "            (0,4,8),(2,4,6)\n",
    "        ]\n",
    "        for i,j,k in lines:\n",
    "            s = self.board[i] + self.board[j] + self.board[k]\n",
    "            if s == 3:  return 1.0, True\n",
    "            if s == -3: return -1.0, True\n",
    "        if np.all(self.board != 0):\n",
    "            return 0.0, True\n",
    "        return 0.0, False\n",
    "\n",
    "    def step(self, action, opponent_policy=None):\n",
    "        if self.board[action] != 0:\n",
    "            return self.board.copy(), -1.0, True\n",
    "\n",
    "        self.board[action] = self.agent\n",
    "        r, d = self.reward_done()\n",
    "        if d:\n",
    "            return self.board.copy(), r, True\n",
    "\n",
    "        if opponent_policy is None:\n",
    "            opp_action = np.random.choice(self.available_actions())\n",
    "        else:\n",
    "            opp_action = opponent_policy(self.board.copy(), self.available_actions())\n",
    "\n",
    "        self.board[opp_action] = self.opponent\n",
    "        r, d = self.reward_done()\n",
    "\n",
    "        if d and r == -1:\n",
    "            return self.board.copy(), -1.0, True\n",
    "\n",
    "        return self.board.copy(), r, d\n",
    "\n",
    "# --- 2. Model ---\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(9, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 9)\n",
    "        )\n",
    "        self._init()\n",
    "\n",
    "    def _init(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
    "                nn.init.zeros_(m.bias)\n",
    "        nn.init.uniform_(self.net[-1].weight, -1e-3, 1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. Helpers ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=50000):\n",
    "        self.buf = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, s, a, r, ns, d, next_avail):\n",
    "        self.buf.append((s, a, r, ns, d, next_avail))\n",
    "\n",
    "    def sample(self, batch):\n",
    "        samples = random.sample(self.buf, batch)\n",
    "        s,a,r,ns,d,na = zip(*samples)\n",
    "        return (\n",
    "            torch.FloatTensor(s),\n",
    "            torch.LongTensor(a).unsqueeze(1),\n",
    "            torch.FloatTensor(r).unsqueeze(1),\n",
    "            torch.FloatTensor(ns),\n",
    "            torch.FloatTensor(d).unsqueeze(1),\n",
    "            na\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buf)\n",
    "\n",
    "\n",
    "def select_action(model, state, avail, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(avail)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q = model(torch.FloatTensor(state).unsqueeze(0))[0]\n",
    "\n",
    "    mask = torch.full_like(q, -1e9)\n",
    "    mask[avail] = q[avail]\n",
    "    return mask.argmax().item()\n",
    "\n",
    "def train_step(model, target, optim, buffer, batch=128, gamma=0.95):\n",
    "    if len(buffer) < batch:\n",
    "        return None  # <- important for clean stats handling\n",
    "\n",
    "    s, a, r, ns, d, next_avail = buffer.sample(batch)\n",
    "\n",
    "    q_sa = model(s).gather(1, a)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_next = target(ns)\n",
    "        masked = torch.full_like(q_next, -1e9)\n",
    "        for i, avail in enumerate(next_avail):\n",
    "            masked[i, avail] = q_next[i, avail]\n",
    "\n",
    "        q_max = masked.max(1, keepdim=True)[0]\n",
    "        target_q = r + gamma * q_max * (1 - d)\n",
    "\n",
    "    td_error = target_q - q_sa\n",
    "    loss = nn.SmoothL1Loss()(q_sa, target_q)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_value_(model.parameters(), 1.0)\n",
    "    optim.step()\n",
    "\n",
    "    # ---- stats returned here ----\n",
    "    return {\n",
    "        \"loss\": loss.item(),\n",
    "        \"td_error\": td_error.abs().mean().item(),\n",
    "        \"q_mean\": q_sa.mean().item(),\n",
    "    }\n",
    "\n",
    "def train():\n",
    "    env = TicTacToeEnv()\n",
    "    model = QNetwork()\n",
    "    target = QNetwork()\n",
    "    target.load_state_dict(model.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    buffer = ReplayBuffer()\n",
    "\n",
    "    steps = 0\n",
    "    epsilon = 1.0\n",
    "\n",
    "    losses = []\n",
    "    ep_lengths = []\n",
    "\n",
    "    def update_target():\n",
    "        target.load_state_dict(model.state_dict())\n",
    "\n",
    "    for episode in range(60000):\n",
    "        s = env.reset()\n",
    "        ep_len = 0\n",
    "\n",
    "        if env.mover == -1:\n",
    "            env.board[random.choice(env.available_actions())] = -1\n",
    "            s = env.board.copy()\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            avail = env.available_actions()\n",
    "            epsilon = max(0.01, 1.0 - steps / 40000) if episode < 50000 else 0.005\n",
    "\n",
    "            if episode > 30000:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 1e-4\n",
    "\n",
    "            a = select_action(model, s, avail, epsilon)\n",
    "            ns, r, done = env.step(a)\n",
    "            next_avail = env.available_actions() if not done else []\n",
    "\n",
    "            buffer.push(s, a, r, ns, done, next_avail)\n",
    "            s = ns\n",
    "            steps += 1\n",
    "            ep_len += 1\n",
    "\n",
    "            stats = train_step(model, target, optimizer, buffer)\n",
    "\n",
    "            if stats is not None:\n",
    "                losses.append(stats[\"loss\"])\n",
    "\n",
    "        ep_lengths.append(ep_len)\n",
    "\n",
    "        # --- periodic target update\n",
    "        if episode % 500 == 0:\n",
    "            update_target()\n",
    "\n",
    "        # --- stats print\n",
    "        if episode % 2000 == 0 and episode > 0:\n",
    "            w, d, l = evaluate_vs_random(model)\n",
    "\n",
    "            print(\n",
    "                f\"[Ep {episode:6d}] \"\n",
    "                f\"ε={epsilon:.2f} | \"\n",
    "                f\"Win={w:.2%} Draw={d:.2%} Loss={l:.2%} | \"\n",
    "                f\"AvgLen={np.mean(ep_lengths[-2000:]):.2f} | \"\n",
    "                f\"Loss={np.mean(losses[-5000:]):.4f}\"\n",
    "            )\n",
    "\n",
    "    return model, target, optimizer, buffer\n",
    "\n",
    "def evaluate_vs_random(model, games=500):\n",
    "    env = TicTacToeEnv()\n",
    "    wins = draws = losses = 0\n",
    "\n",
    "    for _ in range(games):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "\n",
    "        if env.mover == -1:\n",
    "            env.board[random.choice(env.available_actions())] = -1\n",
    "            s = env.board.copy()\n",
    "\n",
    "        while not done:\n",
    "            a = select_action(model, s, env.available_actions(), epsilon=0.0)\n",
    "            s, r, done = env.step(a)\n",
    "\n",
    "        if r == 1: wins += 1\n",
    "        elif r == 0: draws += 1\n",
    "        else: losses += 1\n",
    "\n",
    "    return wins / games, draws / games, losses / games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4fd857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep   2000] ε=0.81 | Win=84.80% Draw=5.80% Loss=9.40% | AvgLen=3.80 | Loss=0.0554\n",
      "[Ep   4000] ε=0.62 | Win=89.40% Draw=3.00% Loss=7.60% | AvgLen=3.73 | Loss=0.0696\n",
      "[Ep   6000] ε=0.44 | Win=93.60% Draw=2.60% Loss=3.80% | AvgLen=3.67 | Loss=0.0643\n",
      "[Ep   8000] ε=0.26 | Win=89.20% Draw=4.20% Loss=6.60% | AvgLen=3.58 | Loss=0.0598\n",
      "[Ep  10000] ε=0.09 | Win=87.20% Draw=6.40% Loss=6.40% | AvgLen=3.49 | Loss=0.0569\n",
      "[Ep  12000] ε=0.01 | Win=94.20% Draw=2.40% Loss=3.40% | AvgLen=3.46 | Loss=0.0525\n",
      "[Ep  14000] ε=0.01 | Win=91.60% Draw=3.00% Loss=5.40% | AvgLen=3.44 | Loss=0.0475\n",
      "[Ep  16000] ε=0.01 | Win=92.00% Draw=3.40% Loss=4.60% | AvgLen=3.43 | Loss=0.0430\n",
      "[Ep  18000] ε=0.01 | Win=93.00% Draw=4.00% Loss=3.00% | AvgLen=3.40 | Loss=0.0369\n",
      "[Ep  20000] ε=0.01 | Win=92.60% Draw=5.00% Loss=2.40% | AvgLen=3.44 | Loss=0.0297\n",
      "[Ep  22000] ε=0.01 | Win=94.00% Draw=3.00% Loss=3.00% | AvgLen=3.42 | Loss=0.0245\n",
      "[Ep  24000] ε=0.01 | Win=91.20% Draw=2.40% Loss=6.40% | AvgLen=3.42 | Loss=0.0206\n",
      "[Ep  26000] ε=0.01 | Win=95.40% Draw=3.80% Loss=0.80% | AvgLen=3.41 | Loss=0.0187\n",
      "[Ep  28000] ε=0.01 | Win=90.20% Draw=6.80% Loss=3.00% | AvgLen=3.41 | Loss=0.0166\n",
      "[Ep  30000] ε=0.01 | Win=94.60% Draw=3.00% Loss=2.40% | AvgLen=3.39 | Loss=0.0155\n",
      "[Ep  32000] ε=0.01 | Win=95.80% Draw=2.20% Loss=2.00% | AvgLen=3.32 | Loss=0.0141\n",
      "[Ep  34000] ε=0.01 | Win=95.80% Draw=2.20% Loss=2.00% | AvgLen=3.28 | Loss=0.0131\n",
      "[Ep  36000] ε=0.01 | Win=93.60% Draw=5.00% Loss=1.40% | AvgLen=3.37 | Loss=0.0129\n",
      "[Ep  38000] ε=0.01 | Win=97.20% Draw=2.20% Loss=0.60% | AvgLen=3.32 | Loss=0.0122\n",
      "[Ep  40000] ε=0.01 | Win=95.00% Draw=3.80% Loss=1.20% | AvgLen=3.31 | Loss=0.0113\n",
      "[Ep  42000] ε=0.01 | Win=96.40% Draw=2.80% Loss=0.80% | AvgLen=3.28 | Loss=0.0104\n",
      "[Ep  44000] ε=0.01 | Win=96.40% Draw=2.00% Loss=1.60% | AvgLen=3.26 | Loss=0.0098\n",
      "[Ep  46000] ε=0.01 | Win=92.80% Draw=4.80% Loss=2.40% | AvgLen=3.26 | Loss=0.0094\n",
      "[Ep  48000] ε=0.01 | Win=95.40% Draw=2.80% Loss=1.80% | AvgLen=3.29 | Loss=0.0093\n",
      "[Ep  50000] ε=0.01 | Win=93.60% Draw=4.20% Loss=2.20% | AvgLen=3.33 | Loss=0.0098\n",
      "[Ep  52000] ε=0.01 | Win=94.40% Draw=4.40% Loss=1.20% | AvgLen=3.33 | Loss=0.0095\n",
      "[Ep  54000] ε=0.01 | Win=95.80% Draw=3.20% Loss=1.00% | AvgLen=3.30 | Loss=0.0097\n",
      "[Ep  56000] ε=0.01 | Win=97.40% Draw=2.20% Loss=0.40% | AvgLen=3.28 | Loss=0.0095\n",
      "[Ep  58000] ε=0.01 | Win=95.40% Draw=3.60% Loss=1.00% | AvgLen=3.28 | Loss=0.0097\n"
     ]
    }
   ],
   "source": [
    "final_model, target, optimizer, buffer  = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90790627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resume(model, target, optimizer, buffer, start_episode=60000, num_episodes=10000):\n",
    "    \"\"\"Resume training from a checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained QNetwork\n",
    "        target: The target QNetwork\n",
    "        optimizer: The optimizer state (or create new)\n",
    "        buffer: The replay buffer (can be empty or pre-filled)\n",
    "        start_episode: Episode number to resume from\n",
    "        num_episodes: How many more episodes to train\n",
    "    \"\"\"\n",
    "    env = TicTacToeEnv()\n",
    "    \n",
    "    steps = 0\n",
    "    epsilon = 1.0\n",
    "    \n",
    "    losses = []\n",
    "    ep_lengths = []\n",
    "    \n",
    "    def update_target():\n",
    "        target.load_state_dict(model.state_dict())\n",
    "    \n",
    "    for episode in range(start_episode, start_episode + num_episodes):\n",
    "        s = env.reset()\n",
    "        ep_len = 0\n",
    "        \n",
    "        if env.mover == -1:\n",
    "            env.board[random.choice(env.available_actions())] = -1\n",
    "            s = env.board.copy()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            avail = env.available_actions()\n",
    "            epsilon = max(0.01, 1.0 - steps / 40000) if episode < 50000 else 0.005\n",
    "            \n",
    "            if episode > 30000:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 1e-4\n",
    "            if episode > 50000:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 1e-5\n",
    "            \n",
    "            a = select_action(model, s, avail, epsilon)\n",
    "            ns, r, done = env.step(a)\n",
    "            next_avail = env.available_actions() if not done else []\n",
    "            \n",
    "            buffer.push(s, a, r, ns, done, next_avail)\n",
    "            s = ns\n",
    "            steps += 1\n",
    "            ep_len += 1\n",
    "            \n",
    "            stats = train_step(model, target, optimizer, buffer)\n",
    "            \n",
    "            if stats is not None:\n",
    "                losses.append(stats[\"loss\"])\n",
    "        \n",
    "        ep_lengths.append(ep_len)\n",
    "        \n",
    "        if episode % 500 == 0:\n",
    "            update_target()\n",
    "        \n",
    "        if episode % 2000 == 0 and episode > 0:\n",
    "            w, d, l = evaluate_vs_random(model)\n",
    "            \n",
    "            print(\n",
    "                f\"[Ep {episode:6d}] \"\n",
    "                f\"ε={epsilon:.2f} | \"\n",
    "                f\"Win={w:.2%} Draw={d:.2%} Loss={l:.2%} | \"\n",
    "                f\"AvgLen={np.mean(ep_lengths[-2000:]):.2f} | \"\n",
    "                f\"Loss={np.mean(losses[-5000:]):.4f}\"\n",
    "            )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d785e70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep  70000] ε=0.01 | Win=97.20% Draw=2.20% Loss=0.60% | AvgLen=4.00 | Loss=0.0060\n",
      "[Ep  72000] ε=0.01 | Win=96.80% Draw=3.20% Loss=0.00% | AvgLen=3.25 | Loss=0.0088\n",
      "[Ep  74000] ε=0.01 | Win=95.60% Draw=4.20% Loss=0.20% | AvgLen=3.20 | Loss=0.0084\n",
      "[Ep  76000] ε=0.01 | Win=94.60% Draw=3.40% Loss=2.00% | AvgLen=3.27 | Loss=0.0082\n",
      "[Ep  78000] ε=0.01 | Win=95.40% Draw=3.20% Loss=1.40% | AvgLen=3.32 | Loss=0.0086\n"
     ]
    }
   ],
   "source": [
    "final_model = train_resume(final_model, target, optimizer, buffer, start_episode=70000, num_episodes=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a3d3fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved as 'model_v8.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(final_model.state_dict(), 'model_v8.pth')\n",
    "print(\"\\nModel saved as 'model_v8.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
