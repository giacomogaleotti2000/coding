{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154f02b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# --- 1. Environment ---\n",
    "class TicTacToeEnv:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros(9, dtype=np.int8)\n",
    "        self.agent = 1\n",
    "        self.opponent = -1\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.board[:] = 0\n",
    "        self.mover = np.random.choice([1, -1])\n",
    "        return self.board.copy()\n",
    "\n",
    "    def available_actions(self):\n",
    "        return np.where(self.board == 0)[0]\n",
    "\n",
    "    def reward_done(self):\n",
    "        lines = [\n",
    "            (0,1,2),(3,4,5),(6,7,8),\n",
    "            (0,3,6),(1,4,7),(2,5,8),\n",
    "            (0,4,8),(2,4,6)\n",
    "        ]\n",
    "        for i,j,k in lines:\n",
    "            s = self.board[i] + self.board[j] + self.board[k]\n",
    "            if s == 3:  return 1.0, True\n",
    "            if s == -3: return -1.0, True\n",
    "        if np.all(self.board != 0):\n",
    "            return 0.0, True\n",
    "        return 0.0, False\n",
    "\n",
    "    def step(self, action, opponent_policy=None):\n",
    "        if self.board[action] != 0:\n",
    "            return self.board.copy(), -1.0, True\n",
    "\n",
    "        self.board[action] = self.agent\n",
    "        r, d = self.reward_done()\n",
    "        if d:\n",
    "            return self.board.copy(), r, True\n",
    "\n",
    "        if opponent_policy is None:\n",
    "            opp_action = np.random.choice(self.available_actions())\n",
    "        else:\n",
    "            opp_action = opponent_policy(self.board.copy(), self.available_actions())\n",
    "\n",
    "        self.board[opp_action] = self.opponent\n",
    "        r, d = self.reward_done()\n",
    "\n",
    "        if d and r == -1:\n",
    "            return self.board.copy(), -1.0, True\n",
    "\n",
    "        return self.board.copy(), r, d\n",
    "\n",
    "# --- 2. Model ---\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(9, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 9)\n",
    "        )\n",
    "        self._init()\n",
    "\n",
    "    def _init(self):\n",
    "        for m in self.net:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity=\"relu\")\n",
    "                nn.init.zeros_(m.bias)\n",
    "        nn.init.uniform_(self.net[-1].weight, -1e-3, 1e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "# --- 3. Helpers ---\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=50000):\n",
    "        self.buf = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, s, a, r, ns, d, next_avail):\n",
    "        self.buf.append((s, a, r, ns, d, next_avail))\n",
    "\n",
    "    def sample(self, batch):\n",
    "        samples = random.sample(self.buf, batch)\n",
    "        s,a,r,ns,d,na = zip(*samples)\n",
    "        return (\n",
    "            torch.FloatTensor(s),\n",
    "            torch.LongTensor(a).unsqueeze(1),\n",
    "            torch.FloatTensor(r).unsqueeze(1),\n",
    "            torch.FloatTensor(ns),\n",
    "            torch.FloatTensor(d).unsqueeze(1),\n",
    "            na\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buf)\n",
    "\n",
    "\n",
    "def select_action(model, state, avail, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(avail)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q = model(torch.FloatTensor(state).unsqueeze(0))[0]\n",
    "\n",
    "    mask = torch.full_like(q, -1e9)\n",
    "    mask[avail] = q[avail]\n",
    "    return mask.argmax().item()\n",
    "\n",
    "def train_step(model, target, optim, buffer, batch=128, gamma=0.95):\n",
    "    if len(buffer) < batch:\n",
    "        return None  # <- important for clean stats handling\n",
    "\n",
    "    s, a, r, ns, d, next_avail = buffer.sample(batch)\n",
    "\n",
    "    q_sa = model(s).gather(1, a)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        q_next = target(ns)\n",
    "        masked = torch.full_like(q_next, -1e9)\n",
    "        for i, avail in enumerate(next_avail):\n",
    "            masked[i, avail] = q_next[i, avail]\n",
    "\n",
    "        q_max = masked.max(1, keepdim=True)[0]\n",
    "        target_q = r + gamma * q_max * (1 - d)\n",
    "\n",
    "    td_error = target_q - q_sa\n",
    "    loss = nn.SmoothL1Loss()(q_sa, target_q)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_value_(model.parameters(), 1.0)\n",
    "    optim.step()\n",
    "\n",
    "    # ---- stats returned here ----\n",
    "    return {\n",
    "        \"loss\": loss.item(),\n",
    "        \"td_error\": td_error.abs().mean().item(),\n",
    "        \"q_mean\": q_sa.mean().item(),\n",
    "    }\n",
    "\n",
    "def train():\n",
    "    env = TicTacToeEnv()\n",
    "    model = QNetwork()\n",
    "    target = QNetwork()\n",
    "    target.load_state_dict(model.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    buffer = ReplayBuffer()\n",
    "\n",
    "    steps = 0\n",
    "    epsilon = 1.0\n",
    "\n",
    "    losses = []\n",
    "    ep_lengths = []\n",
    "\n",
    "    def update_target():\n",
    "        target.load_state_dict(model.state_dict())\n",
    "\n",
    "    for episode in range(70000):\n",
    "        s = env.reset()\n",
    "        ep_len = 0\n",
    "\n",
    "        if env.mover == -1:\n",
    "            env.board[random.choice(env.available_actions())] = -1\n",
    "            s = env.board.copy()\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            avail = env.available_actions()\n",
    "            epsilon = max(0.01, 1.0 - steps / 40000) if episode < 50000 else 0.001\n",
    "\n",
    "            if episode > 30000:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 1e-4\n",
    "            if episode > 50000:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 1e-5\n",
    "\n",
    "            a = select_action(model, s, avail, epsilon)\n",
    "            ns, r, done = env.step(a)\n",
    "            next_avail = env.available_actions() if not done else []\n",
    "\n",
    "            buffer.push(s, a, r, ns, done, next_avail)\n",
    "            s = ns\n",
    "            steps += 1\n",
    "            ep_len += 1\n",
    "\n",
    "            stats = train_step(model, target, optimizer, buffer)\n",
    "\n",
    "            if stats is not None:\n",
    "                losses.append(stats[\"loss\"])\n",
    "\n",
    "        ep_lengths.append(ep_len)\n",
    "\n",
    "        # --- periodic target update\n",
    "        if episode % 500 == 0:\n",
    "            update_target()\n",
    "\n",
    "        # --- stats print\n",
    "        if episode % 2000 == 0 and episode > 0:\n",
    "            w, d, l = evaluate_vs_random(model)\n",
    "\n",
    "            print(\n",
    "                f\"[Ep {episode:6d}] \"\n",
    "                f\"ε={epsilon:.2f} | \"\n",
    "                f\"Win={w:.2%} Draw={d:.2%} Loss={l:.2%} | \"\n",
    "                f\"AvgLen={np.mean(ep_lengths[-2000:]):.2f} | \"\n",
    "                f\"Loss={np.mean(losses[-5000:]):.4f}\"\n",
    "            )\n",
    "\n",
    "    return model, target, optimizer, buffer\n",
    "\n",
    "def evaluate_vs_random(model, games=500):\n",
    "    env = TicTacToeEnv()\n",
    "    wins = draws = losses = 0\n",
    "\n",
    "    for _ in range(games):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "\n",
    "        if env.mover == -1:\n",
    "            env.board[random.choice(env.available_actions())] = -1\n",
    "            s = env.board.copy()\n",
    "\n",
    "        while not done:\n",
    "            a = select_action(model, s, env.available_actions(), epsilon=0.0)\n",
    "            s, r, done = env.step(a)\n",
    "\n",
    "        if r == 1: wins += 1\n",
    "        elif r == 0: draws += 1\n",
    "        else: losses += 1\n",
    "\n",
    "    return wins / games, draws / games, losses / games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4fd857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wm/j55npq691qg1cdlyt0lmy0qm0000gn/T/ipykernel_80076/1505410944.py:97: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
      "  torch.FloatTensor(s),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep   2000] ε=0.81 | Win=86.60% Draw=5.20% Loss=8.20% | AvgLen=3.79 | Loss=0.0544\n",
      "[Ep   4000] ε=0.62 | Win=90.80% Draw=4.60% Loss=4.60% | AvgLen=3.73 | Loss=0.0631\n",
      "[Ep   6000] ε=0.44 | Win=90.80% Draw=4.60% Loss=4.60% | AvgLen=3.66 | Loss=0.0631\n",
      "[Ep   8000] ε=0.26 | Win=91.40% Draw=5.60% Loss=3.00% | AvgLen=3.62 | Loss=0.0569\n",
      "[Ep  10000] ε=0.08 | Win=91.40% Draw=3.00% Loss=5.60% | AvgLen=3.58 | Loss=0.0532\n",
      "[Ep  12000] ε=0.01 | Win=90.60% Draw=4.60% Loss=4.80% | AvgLen=3.49 | Loss=0.0475\n",
      "[Ep  14000] ε=0.01 | Win=87.40% Draw=5.60% Loss=7.00% | AvgLen=3.44 | Loss=0.0445\n",
      "[Ep  16000] ε=0.01 | Win=92.80% Draw=4.00% Loss=3.20% | AvgLen=3.44 | Loss=0.0385\n",
      "[Ep  18000] ε=0.01 | Win=92.60% Draw=3.80% Loss=3.60% | AvgLen=3.50 | Loss=0.0315\n",
      "[Ep  20000] ε=0.01 | Win=94.60% Draw=2.80% Loss=2.60% | AvgLen=3.47 | Loss=0.0260\n",
      "[Ep  22000] ε=0.01 | Win=93.80% Draw=4.00% Loss=2.20% | AvgLen=3.44 | Loss=0.0211\n",
      "[Ep  24000] ε=0.01 | Win=93.20% Draw=2.80% Loss=4.00% | AvgLen=3.38 | Loss=0.0182\n",
      "[Ep  26000] ε=0.01 | Win=94.00% Draw=3.40% Loss=2.60% | AvgLen=3.40 | Loss=0.0176\n",
      "[Ep  28000] ε=0.01 | Win=93.40% Draw=3.80% Loss=2.80% | AvgLen=3.43 | Loss=0.0158\n",
      "[Ep  30000] ε=0.01 | Win=94.20% Draw=3.60% Loss=2.20% | AvgLen=3.43 | Loss=0.0150\n",
      "[Ep  32000] ε=0.01 | Win=95.20% Draw=3.80% Loss=1.00% | AvgLen=3.40 | Loss=0.0142\n",
      "[Ep  34000] ε=0.01 | Win=95.40% Draw=3.40% Loss=1.20% | AvgLen=3.32 | Loss=0.0139\n",
      "[Ep  36000] ε=0.01 | Win=94.40% Draw=3.60% Loss=2.00% | AvgLen=3.31 | Loss=0.0131\n",
      "[Ep  38000] ε=0.01 | Win=94.20% Draw=4.20% Loss=1.60% | AvgLen=3.30 | Loss=0.0125\n",
      "[Ep  40000] ε=0.01 | Win=96.20% Draw=1.60% Loss=2.20% | AvgLen=3.30 | Loss=0.0119\n",
      "[Ep  42000] ε=0.01 | Win=95.80% Draw=3.00% Loss=1.20% | AvgLen=3.31 | Loss=0.0109\n",
      "[Ep  44000] ε=0.01 | Win=97.00% Draw=2.00% Loss=1.00% | AvgLen=3.30 | Loss=0.0105\n",
      "[Ep  46000] ε=0.01 | Win=94.60% Draw=3.80% Loss=1.60% | AvgLen=3.29 | Loss=0.0101\n",
      "[Ep  48000] ε=0.01 | Win=95.00% Draw=4.40% Loss=0.60% | AvgLen=3.32 | Loss=0.0097\n",
      "[Ep  50000] ε=0.00 | Win=94.20% Draw=4.40% Loss=1.40% | AvgLen=3.32 | Loss=0.0095\n",
      "[Ep  52000] ε=0.00 | Win=96.40% Draw=2.20% Loss=1.40% | AvgLen=3.30 | Loss=0.0095\n",
      "[Ep  54000] ε=0.00 | Win=95.20% Draw=3.40% Loss=1.40% | AvgLen=3.33 | Loss=0.0091\n",
      "[Ep  56000] ε=0.00 | Win=94.80% Draw=4.60% Loss=0.60% | AvgLen=3.28 | Loss=0.0090\n",
      "[Ep  58000] ε=0.00 | Win=97.00% Draw=3.00% Loss=0.00% | AvgLen=3.25 | Loss=0.0087\n",
      "[Ep  60000] ε=0.00 | Win=96.00% Draw=3.60% Loss=0.40% | AvgLen=3.24 | Loss=0.0084\n",
      "[Ep  62000] ε=0.00 | Win=97.80% Draw=2.00% Loss=0.20% | AvgLen=3.27 | Loss=0.0085\n",
      "[Ep  64000] ε=0.00 | Win=96.40% Draw=3.20% Loss=0.40% | AvgLen=3.24 | Loss=0.0078\n",
      "[Ep  66000] ε=0.00 | Win=95.40% Draw=4.20% Loss=0.40% | AvgLen=3.23 | Loss=0.0075\n",
      "[Ep  68000] ε=0.00 | Win=96.40% Draw=3.40% Loss=0.20% | AvgLen=3.24 | Loss=0.0074\n"
     ]
    }
   ],
   "source": [
    "final_model, target, optimizer, buffer  = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90790627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resume(model, target, optimizer, buffer, start_episode=60000, num_episodes=10000):\n",
    "    \"\"\"Resume training from a checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained QNetwork\n",
    "        target: The target QNetwork\n",
    "        optimizer: The optimizer state (or create new)\n",
    "        buffer: The replay buffer (can be empty or pre-filled)\n",
    "        start_episode: Episode number to resume from\n",
    "        num_episodes: How many more episodes to train\n",
    "    \"\"\"\n",
    "    env = TicTacToeEnv()\n",
    "    \n",
    "    steps = 0\n",
    "    epsilon = 1.0\n",
    "    \n",
    "    losses = []\n",
    "    ep_lengths = []\n",
    "    \n",
    "    def update_target():\n",
    "        target.load_state_dict(model.state_dict())\n",
    "    \n",
    "    for episode in range(start_episode, start_episode + num_episodes):\n",
    "        s = env.reset()\n",
    "        ep_len = 0\n",
    "        \n",
    "        if env.mover == -1:\n",
    "            env.board[random.choice(env.available_actions())] = -1\n",
    "            s = env.board.copy()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            avail = env.available_actions()\n",
    "            epsilon = max(0.01, 1.0 - steps / 40000) if episode < 50000 else 0.0\n",
    "            \n",
    "            if episode > 30000:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 1e-4\n",
    "            if episode > 50000:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = 1e-6\n",
    "            \n",
    "            a = select_action(model, s, avail, epsilon)\n",
    "            ns, r, done = env.step(a)\n",
    "            next_avail = env.available_actions() if not done else []\n",
    "            \n",
    "            buffer.push(s, a, r, ns, done, next_avail)\n",
    "            s = ns\n",
    "            steps += 1\n",
    "            ep_len += 1\n",
    "            \n",
    "            stats = train_step(model, target, optimizer, buffer)\n",
    "            \n",
    "            if stats is not None:\n",
    "                losses.append(stats[\"loss\"])\n",
    "        \n",
    "        ep_lengths.append(ep_len)\n",
    "        \n",
    "        if episode % 500 == 0:\n",
    "            update_target()\n",
    "        \n",
    "        if episode % 2000 == 0 and episode > 0:\n",
    "            w, d, l = evaluate_vs_random(model)\n",
    "            \n",
    "            print(\n",
    "                f\"[Ep {episode:6d}] \"\n",
    "                f\"ε={epsilon:.2f} | \"\n",
    "                f\"Win={w:.2%} Draw={d:.2%} Loss={l:.2%} | \"\n",
    "                f\"AvgLen={np.mean(ep_lengths[-2000:]):.2f} | \"\n",
    "                f\"Loss={np.mean(losses[-5000:]):.4f}\"\n",
    "            )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d785e70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ep 2000000] ε=0.00 | Win=96.60% Draw=3.40% Loss=0.00% | AvgLen=3.00 | Loss=0.0049\n",
      "[Ep 2002000] ε=0.00 | Win=96.00% Draw=3.20% Loss=0.80% | AvgLen=3.23 | Loss=0.0066\n",
      "[Ep 2004000] ε=0.00 | Win=96.80% Draw=2.80% Loss=0.40% | AvgLen=3.25 | Loss=0.0065\n",
      "[Ep 2006000] ε=0.00 | Win=95.80% Draw=3.80% Loss=0.40% | AvgLen=3.24 | Loss=0.0064\n",
      "[Ep 2008000] ε=0.00 | Win=95.80% Draw=4.00% Loss=0.20% | AvgLen=3.23 | Loss=0.0061\n",
      "[Ep 2010000] ε=0.00 | Win=96.00% Draw=4.00% Loss=0.00% | AvgLen=3.23 | Loss=0.0061\n",
      "[Ep 2012000] ε=0.00 | Win=94.00% Draw=5.80% Loss=0.20% | AvgLen=3.23 | Loss=0.0061\n",
      "[Ep 2014000] ε=0.00 | Win=95.00% Draw=4.80% Loss=0.20% | AvgLen=3.25 | Loss=0.0062\n",
      "[Ep 2016000] ε=0.00 | Win=96.80% Draw=3.20% Loss=0.00% | AvgLen=3.22 | Loss=0.0060\n",
      "[Ep 2018000] ε=0.00 | Win=96.60% Draw=3.40% Loss=0.00% | AvgLen=3.23 | Loss=0.0059\n"
     ]
    }
   ],
   "source": [
    "final_model = train_resume(final_model, target, optimizer, buffer, start_episode=2000000, num_episodes=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a3d3fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved as 'model_v9.pth'\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(final_model.state_dict(), 'model_v9.pth')\n",
    "print(\"\\nModel saved as 'model_v9.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdc14db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
