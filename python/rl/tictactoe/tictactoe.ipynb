{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5c4cf656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe import TicTacToeEnv, QNTicTacToe\n",
    "import numpy as np\n",
    "\n",
    "INIT_RANDOM_GAMES = 500\n",
    "BATCH = 64\n",
    "EPOCHS = 10\n",
    "GAMMA = 0.9\n",
    "\n",
    "model = QNTicTacToe()\n",
    "env = TicTacToeEnv()\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "next_states = []\n",
    "rewards = []\n",
    "dones = []\n",
    "\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "558aed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(INIT_RANDOM_GAMES):\n",
    "    env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        s = env.board.copy()\n",
    "        a = env.random_action()\n",
    "        ns, r, done = env.step(a)\n",
    "\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        next_states.append(ns)\n",
    "        rewards.append(r)\n",
    "        dones.append(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9612849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, n_games=100):\n",
    "    \"\"\"Play against random opponent and see win rate\"\"\"\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            s = env.board.copy()\n",
    "            with torch.no_grad():\n",
    "                q_vals = model(torch.tensor(s, dtype=torch.float32))\n",
    "                # Mask illegal moves\n",
    "                available_mask = (s == 0)\n",
    "                q_vals[~available_mask] = -float('inf')\n",
    "                a = int(torch.argmax(q_vals).item())\n",
    "            \n",
    "            _, r, done = env.step(a)\n",
    "        \n",
    "        if r == 1:\n",
    "            wins += 1\n",
    "        elif r == -1:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    return wins/n_games*100, losses/n_games*100, draws/n_games*100\n",
    "\n",
    "def train_n_games(model, target_model, optimizer, n_games=20000+1, epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=0.9995):\n",
    "    \n",
    "    epsilon = epsilon_start\n",
    "    losses_all = []\n",
    "\n",
    "    for game in range(n_games):\n",
    "\n",
    "        # --- play one game ---\n",
    "        env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            s = env.board.copy()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q_vals = model(torch.tensor(s, dtype=torch.float32))\n",
    "\n",
    "            # epsilon-greedy\n",
    "            if random.random() < epsilon:\n",
    "                a = int(np.random.choice(np.where(s == 0)[0]))\n",
    "            else:\n",
    "                a = int(torch.argmax(q_vals).item())\n",
    "\n",
    "            ns, r, done = env.step(a)\n",
    "\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            next_states.append(ns)\n",
    "            rewards.append(r)\n",
    "            dones.append(done)\n",
    "\n",
    "        # decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        # update target network every 1000 games\n",
    "        if game % 500 == 0 and game > 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        # --- training step ---\n",
    "        states_t      = torch.tensor(states,      dtype=torch.float32)\n",
    "        actions_t     = torch.tensor(actions,     dtype=torch.int64)\n",
    "        next_states_t = torch.tensor(next_states, dtype=torch.float32)\n",
    "        rewards_t     = torch.tensor(rewards,     dtype=torch.float32)\n",
    "        dones_t       = torch.tensor(dones,       dtype=torch.float32)\n",
    "\n",
    "        steps = len(states_t) // BATCH // 10\n",
    "\n",
    "        for _ in range(steps):\n",
    "            idx = np.random.randint(0, len(states_t), size=BATCH)\n",
    "\n",
    "            batch_s  = states_t[idx]\n",
    "            batch_a  = actions_t[idx]\n",
    "            batch_ns = next_states_t[idx]\n",
    "            batch_r  = rewards_t[idx]\n",
    "            batch_d  = dones_t[idx]\n",
    "\n",
    "            # Q(s,a)\n",
    "            q_vals = model(batch_s)\n",
    "            q_sa = q_vals.gather(1, batch_a.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # target\n",
    "            with torch.no_grad():\n",
    "                next_q = target_model(batch_ns)\n",
    "                max_next_q = torch.max(next_q, dim=1).values\n",
    "                target = batch_r + GAMMA * max_next_q * (1 - batch_d)\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(q_sa, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses_all.append(loss.item())\n",
    "\n",
    "        # print progress\n",
    "        if game % 1000 == 0:\n",
    "            w, l, d = evaluate_model(model, n_games=1000)\n",
    "            print(f\"Game {game},  ε={epsilon:.3f}, loss={losses_all[-1]:.4f}, Win rate: {w:.2f}%, Loss: {l:.2f}%, Draw: {d:.2f}%\")\n",
    "\n",
    "    return losses_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "676c6e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0,  ε=1.000, loss=1.1677, Win rate: 55.20%, Loss: 33.40%, Draw: 11.40%\n",
      "Game 1000,  ε=0.606, loss=1.8312, Win rate: 79.20%, Loss: 12.60%, Draw: 8.20%\n",
      "Game 2000,  ε=0.368, loss=1.0649, Win rate: 87.90%, Loss: 11.10%, Draw: 1.00%\n",
      "Game 3000,  ε=0.223, loss=0.8014, Win rate: 91.40%, Loss: 7.20%, Draw: 1.40%\n",
      "Game 4000,  ε=0.135, loss=0.3670, Win rate: 90.40%, Loss: 7.30%, Draw: 2.30%\n",
      "Game 5000,  ε=0.082, loss=0.9235, Win rate: 90.00%, Loss: 7.90%, Draw: 2.10%\n",
      "Game 6000,  ε=0.050, loss=0.2551, Win rate: 90.50%, Loss: 7.30%, Draw: 2.20%\n",
      "Game 7000,  ε=0.050, loss=0.2411, Win rate: 92.00%, Loss: 5.30%, Draw: 2.70%\n",
      "Game 8000,  ε=0.050, loss=0.1349, Win rate: 93.30%, Loss: 2.70%, Draw: 4.00%\n",
      "Game 9000,  ε=0.050, loss=0.0909, Win rate: 92.00%, Loss: 2.90%, Draw: 5.10%\n",
      "Game 10000,  ε=0.050, loss=0.0729, Win rate: 92.70%, Loss: 2.90%, Draw: 4.40%\n",
      "Game 11000,  ε=0.050, loss=0.1217, Win rate: 92.10%, Loss: 3.00%, Draw: 4.90%\n",
      "Game 12000,  ε=0.050, loss=0.1194, Win rate: 93.70%, Loss: 2.80%, Draw: 3.50%\n",
      "Game 13000,  ε=0.050, loss=0.0947, Win rate: 94.80%, Loss: 1.70%, Draw: 3.50%\n",
      "Game 14000,  ε=0.050, loss=0.1370, Win rate: 92.80%, Loss: 2.00%, Draw: 5.20%\n",
      "Game 15000,  ε=0.050, loss=0.0730, Win rate: 94.30%, Loss: 1.60%, Draw: 4.10%\n",
      "Game 16000,  ε=0.050, loss=0.0863, Win rate: 91.80%, Loss: 1.40%, Draw: 6.80%\n",
      "Game 17000,  ε=0.050, loss=0.0488, Win rate: 92.60%, Loss: 3.50%, Draw: 3.90%\n",
      "Game 18000,  ε=0.050, loss=0.0383, Win rate: 95.40%, Loss: 1.30%, Draw: 3.30%\n",
      "Game 19000,  ε=0.050, loss=0.0627, Win rate: 95.00%, Loss: 1.80%, Draw: 3.20%\n",
      "Game 20000,  ε=0.050, loss=0.0563, Win rate: 96.20%, Loss: 0.70%, Draw: 3.10%\n"
     ]
    }
   ],
   "source": [
    "model = QNTicTacToe()\n",
    "target_model = QNTicTacToe()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-5)\n",
    "\n",
    "losses = train_n_games(model, target_model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "369830fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished. Model saved to model_1.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model_1.pth\")\n",
    "print(\"Training finished. Model saved to\", \"model_1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41c7f2",
   "metadata": {},
   "source": [
    "## **Keep training on higher quality games**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "be1e82e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating games with model_3 playing against itself...\n",
      "Generated 100/500 games\n",
      "Generated 200/500 games\n",
      "Generated 300/500 games\n",
      "Generated 400/500 games\n",
      "Generated 500/500 games\n",
      "\n",
      "Collected 1678 transitions from 500 self-play games\n",
      "Average transitions per game: 3.4\n",
      "\n",
      "Game outcomes from Player 1 perspective:\n",
      "Wins: 464, Losses: 19, Draws: 17\n"
     ]
    }
   ],
   "source": [
    "from tictactoe import TicTacToeEnv, QNTicTacToe\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "INIT_SMART_GAMES = 500\n",
    "BATCH = 1000\n",
    "EPOCHS = 10\n",
    "GAMMA = 0.9\n",
    "\n",
    "# Load the trained model\n",
    "model_3 = QNTicTacToe()\n",
    "model_3.load_state_dict(torch.load(\"model_4.pth\"))\n",
    "model_3.eval()\n",
    "\n",
    "env = TicTacToeEnv()\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "next_states = []\n",
    "rewards = []\n",
    "dones = []\n",
    "\n",
    "def select_action_with_model(model, state, epsilon=0.1):\n",
    "    \"\"\"Select action using model with optional epsilon-greedy\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        # Random action\n",
    "        return env.random_action()\n",
    "    else:\n",
    "        # Model action\n",
    "        with torch.no_grad():\n",
    "            q_vals = model(torch.tensor(state, dtype=torch.float32))\n",
    "            available_mask = (state == 0)\n",
    "            q_vals[~available_mask] = -float('inf')\n",
    "            return int(torch.argmax(q_vals).item())\n",
    "\n",
    "print(\"Generating games with model_3 playing against itself...\")\n",
    "for game_idx in range(INIT_SMART_GAMES):\n",
    "    env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        s = env.board.copy()\n",
    "        \n",
    "        # Model plays as current player (with small epsilon for exploration)\n",
    "        a = select_action_with_model(model_3, s, epsilon=0.1)\n",
    "        \n",
    "        ns, r, done = env.step(a)\n",
    "\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        next_states.append(ns)\n",
    "        rewards.append(r)\n",
    "        dones.append(done)\n",
    "    \n",
    "    if (game_idx + 1) % 100 == 0:\n",
    "        print(f\"Generated {game_idx + 1}/{INIT_SMART_GAMES} games\")\n",
    "\n",
    "print(f\"\\nCollected {len(states)} transitions from {INIT_SMART_GAMES} self-play games\")\n",
    "print(f\"Average transitions per game: {len(states)/INIT_SMART_GAMES:.1f}\")\n",
    "\n",
    "# Check quality of generated games\n",
    "wins = sum(1 for r in rewards if r == 1)\n",
    "losses = sum(1 for r in rewards if r == -1)\n",
    "draws = sum(1 for i, r in enumerate(rewards) if r == 0 and dones[i])  # Fix: move enumerate to front\n",
    "print(f\"\\nGame outcomes from Player 1 perspective:\")\n",
    "print(f\"Wins: {wins}, Losses: {losses}, Draws: {draws}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2652d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, n_games=100):\n",
    "    \"\"\"Play against random opponent and see win rate\"\"\"\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            s = env.board.copy()\n",
    "            with torch.no_grad():\n",
    "                q_vals = model(torch.tensor(s, dtype=torch.float32))\n",
    "                # Mask illegal moves\n",
    "                available_mask = (s == 0)\n",
    "                q_vals[~available_mask] = -float('inf')\n",
    "                a = int(torch.argmax(q_vals).item())\n",
    "            \n",
    "            _, r, done = env.step(a)\n",
    "        \n",
    "        if r == 1:\n",
    "            wins += 1\n",
    "        elif r == -1:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    return wins/n_games*100, losses/n_games*100, draws/n_games*100\n",
    "\n",
    "def train_n_games(model, target_model, optimizer, n_games=20000+1, epsilon_start=1.0, epsilon_end=0.05, epsilon_decay=0.9995):\n",
    "    \n",
    "    epsilon = epsilon_start\n",
    "    losses_all = []\n",
    "\n",
    "    for game in range(n_games):\n",
    "\n",
    "        # --- play one game ---\n",
    "        env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            s = env.board.copy()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                q_vals = model(torch.tensor(s, dtype=torch.float32))\n",
    "\n",
    "            # epsilon-greedy\n",
    "            if random.random() < epsilon:\n",
    "                a = int(np.random.choice(np.where(s == 0)[0]))\n",
    "            else:\n",
    "                a = int(torch.argmax(q_vals).item())\n",
    "\n",
    "            ns, r, done = env.step(a)\n",
    "\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            next_states.append(ns)\n",
    "            rewards.append(r)\n",
    "            dones.append(done)\n",
    "\n",
    "        # decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        # update target network every 1000 games\n",
    "        if game % 500 == 0 and game > 0:\n",
    "            target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        # --- training step ---\n",
    "        states_t      = torch.tensor(states,      dtype=torch.float32)\n",
    "        actions_t     = torch.tensor(actions,     dtype=torch.int64)\n",
    "        next_states_t = torch.tensor(next_states, dtype=torch.float32)\n",
    "        rewards_t     = torch.tensor(rewards,     dtype=torch.float32)\n",
    "        dones_t       = torch.tensor(dones,       dtype=torch.float32)\n",
    "\n",
    "        steps = len(states_t) // BATCH\n",
    "\n",
    "        for _ in range(steps):\n",
    "            idx = np.random.randint(0, len(states_t), size=BATCH)\n",
    "\n",
    "            batch_s  = states_t[idx]\n",
    "            batch_a  = actions_t[idx]\n",
    "            batch_ns = next_states_t[idx]\n",
    "            batch_r  = rewards_t[idx]\n",
    "            batch_d  = dones_t[idx]\n",
    "\n",
    "            # Q(s,a)\n",
    "            q_vals = model(batch_s)\n",
    "            q_sa = q_vals.gather(1, batch_a.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            # target\n",
    "            with torch.no_grad():\n",
    "                next_q = target_model(batch_ns)\n",
    "                max_next_q = torch.max(next_q, dim=1).values\n",
    "                target = batch_r + GAMMA * max_next_q * (1 - batch_d)\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(q_sa, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses_all.append(loss.item())\n",
    "\n",
    "        # print progress\n",
    "        if game % 1000 == 0:\n",
    "            w, l, d = evaluate_model(model, n_games=1000)\n",
    "            print(f\"Game {game},  ε={epsilon:.3f}, loss={losses_all[-1]:.4f}, Win rate: {w:.2f}%, Loss: {l:.2f}%, Draw: {d:.2f}%\")\n",
    "\n",
    "    return losses_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9a88b373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f3bb930f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0,  ε=0.900, loss=0.0448, Win rate: 94.90%, Loss: 1.50%, Draw: 3.60%\n",
      "Game 1000,  ε=0.000, loss=0.0326, Win rate: 95.20%, Loss: 2.20%, Draw: 2.60%\n",
      "Game 2000,  ε=0.000, loss=0.0303, Win rate: 95.40%, Loss: 1.60%, Draw: 3.00%\n",
      "Game 3000,  ε=0.000, loss=0.0317, Win rate: 95.30%, Loss: 2.10%, Draw: 2.60%\n",
      "Game 4000,  ε=0.000, loss=0.0371, Win rate: 95.20%, Loss: 2.60%, Draw: 2.20%\n",
      "Game 5000,  ε=0.000, loss=0.0305, Win rate: 95.30%, Loss: 2.90%, Draw: 1.80%\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = QNTicTacToe()\n",
    "model.load_state_dict(torch.load(\"model_4.pth\"))\n",
    "model.eval()  # Optional: set to eval mode first\n",
    "\n",
    "# Initialize target model with the same weights\n",
    "target_model = QNTicTacToe()\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "# Set back to training mode\n",
    "model.train()\n",
    "target_model.eval()  # Target model stays in eval mode\n",
    "\n",
    "# Continue training\n",
    "losses = train_n_games(model, target_model, optimizer, n_games=5000+1, epsilon_start=1, epsilon_end=0.0, epsilon_decay=0.9)\n",
    "\n",
    "# Save again with a new name\n",
    "torch.save(model.state_dict(), \"model_4.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80e9bb",
   "metadata": {},
   "source": [
    "## **Making two models play against each other**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "070ff089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing 2 models...\n",
      "\n",
      "\n",
      "model_4.pth vs model_1.pth\n",
      "\n",
      "============================================================\n",
      "Model 1 vs Model 2 - 10000 games\n",
      "============================================================\n",
      "Model 1 wins: 8826 (88.26%)\n",
      "Model 2 wins:  622 ( 6.22%)\n",
      "Draws:         552 ( 5.52%)\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_4.pth_vs_model_1.pth': {'model1_wins': 8826,\n",
       "  'model2_wins': 622,\n",
       "  'draws': 552,\n",
       "  'model1_win_rate': 88.26,\n",
       "  'model2_win_rate': 6.22,\n",
       "  'draw_rate': 5.52,\n",
       "  'total_games': 10000}}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tictactoe import TicTacToeEnv, QNTicTacToe\n",
    "\n",
    "def evaluate_models(model1, model2, n_games=10000, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate two models playing against each other.\n",
    "    \n",
    "    Args:\n",
    "        model1: First model (plays as player 1)\n",
    "        model2: Second model (plays as player 2)\n",
    "        n_games: Number of games to play\n",
    "        epsilon1: Exploration rate for model1 (0 = greedy)\n",
    "        epsilon2: Exploration rate for model2 (0 = greedy)\n",
    "        verbose: Whether to print results\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with statistics\n",
    "    \"\"\"\n",
    "    env = TicTacToeEnv()\n",
    "    \n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "    \n",
    "    model1_wins = 0\n",
    "    model2_wins = 0\n",
    "    draws = 0\n",
    "    \n",
    "    def select_action(model, state):\n",
    "            with torch.no_grad():\n",
    "                q_vals = model(torch.tensor(state, dtype=torch.float32))\n",
    "                return int(torch.argmax(q_vals).item())\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        env.reset()\n",
    "        done = False\n",
    "        current_player = np.random.choice([-1,1])\n",
    "        \n",
    "        while not done:\n",
    "            s = env.board.copy()\n",
    "            \n",
    "            # Model 1 plays as player 1, Model 2 plays as player 2\n",
    "            if current_player == 1:\n",
    "                a = select_action(model1, s)\n",
    "            else:\n",
    "                a = select_action(model2, -s)\n",
    "            \n",
    "            _, r, done = env.step(a)\n",
    "            \n",
    "            # Switch player for next turn\n",
    "            current_player = -current_player\n",
    "        \n",
    "        # Count results from player 1's perspective (model1)\n",
    "        if r == 1:\n",
    "            model1_wins += 1\n",
    "        elif r == -1:\n",
    "            model2_wins += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    \n",
    "    # Calculate percentages\n",
    "    stats = {\n",
    "        'model1_wins': model1_wins,\n",
    "        'model2_wins': model2_wins,\n",
    "        'draws': draws,\n",
    "        'model1_win_rate': model1_wins / n_games * 100,\n",
    "        'model2_win_rate': model2_wins / n_games * 100,\n",
    "        'draw_rate': draws / n_games * 100,\n",
    "        'total_games': n_games\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Model 1 vs Model 2 - {n_games} games\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Model 1 wins: {model1_wins:4d} ({stats['model1_win_rate']:5.2f}%)\")\n",
    "        print(f\"Model 2 wins: {model2_wins:4d} ({stats['model2_win_rate']:5.2f}%)\")\n",
    "        print(f\"Draws:        {draws:4d} ({stats['draw_rate']:5.2f}%)\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def compare_all_models(model_files, n_games=500):\n",
    "    \"\"\"Compare all models against each other\"\"\"\n",
    "    models = []\n",
    "    names = []\n",
    "    \n",
    "    # Load all models\n",
    "    for file in model_files:\n",
    "        model = QNTicTacToe()\n",
    "        model.load_state_dict(torch.load(file))\n",
    "        models.append(model)\n",
    "        names.append(file)\n",
    "    \n",
    "    print(f\"\\nComparing {len(models)} models...\\n\")\n",
    "    \n",
    "    # Create results matrix\n",
    "    results = {}\n",
    "    \n",
    "    for i, (model1, name1) in enumerate(zip(models, names)):\n",
    "        for j, (model2, name2) in enumerate(zip(models, names)):\n",
    "            if i < j:  # Only compare each pair once\n",
    "                print(f\"\\n{name1} vs {name2}\")\n",
    "                stats = evaluate_models(model1, model2, n_games=n_games, verbose=True)\n",
    "                results[f\"{name1}_vs_{name2}\"] = stats\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Usage:\n",
    "compare_all_models([\"model_4.pth\", \"model_1.pth\"], n_games=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
