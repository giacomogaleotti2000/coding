{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "866f5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replay buffer initialized with 501 transitions.\n",
      "Ep 500/30000 | Steps 1891 | eps 0.764 | avg loss 0.1659\n",
      "  wins 245 (0.49) | draws 52 (0.10) | losses 203 (0.41)\n",
      "Ep 1000/30000 | Steps 3716 | eps 0.535 | avg loss 0.1402\n",
      "  wins 553 (0.55) | draws 101 (0.10) | losses 346 (0.35)\n",
      "Ep 1500/30000 | Steps 5464 | eps 0.317 | avg loss 0.1211\n",
      "  wins 942 (0.63) | draws 135 (0.09) | losses 423 (0.28)\n",
      "Ep 2000/30000 | Steps 7142 | eps 0.107 | avg loss 0.1122\n",
      "  wins 1375 (0.69) | draws 153 (0.08) | losses 472 (0.24)\n",
      "Ep 2500/30000 | Steps 8760 | eps 0.000 | avg loss 0.0952\n",
      "  wins 1844 (0.74) | draws 163 (0.07) | losses 493 (0.20)\n",
      "Ep 3000/30000 | Steps 10376 | eps 0.000 | avg loss 0.0856\n",
      "  wins 2304 (0.77) | draws 176 (0.06) | losses 520 (0.17)\n",
      "Ep 3500/30000 | Steps 12001 | eps 0.000 | avg loss 0.0753\n",
      "  wins 2782 (0.79) | draws 188 (0.05) | losses 530 (0.15)\n",
      "Ep 4000/30000 | Steps 13632 | eps 0.000 | avg loss 0.0728\n",
      "  wins 3252 (0.81) | draws 200 (0.05) | losses 548 (0.14)\n",
      "Ep 4500/30000 | Steps 15252 | eps 0.000 | avg loss 0.0655\n",
      "  wins 3725 (0.83) | draws 212 (0.05) | losses 563 (0.13)\n",
      "Ep 5000/30000 | Steps 16907 | eps 0.000 | avg loss 0.0641\n",
      "  wins 4189 (0.84) | draws 232 (0.05) | losses 579 (0.12)\n",
      "Ep 5500/30000 | Steps 18548 | eps 0.000 | avg loss 0.0672\n",
      "  wins 4663 (0.85) | draws 241 (0.04) | losses 596 (0.11)\n",
      "Ep 6000/30000 | Steps 20228 | eps 0.000 | avg loss 0.0591\n",
      "  wins 5127 (0.85) | draws 253 (0.04) | losses 620 (0.10)\n",
      "Ep 6500/30000 | Steps 21887 | eps 0.000 | avg loss 0.0560\n",
      "  wins 5585 (0.86) | draws 275 (0.04) | losses 640 (0.10)\n",
      "Ep 7000/30000 | Steps 23530 | eps 0.000 | avg loss 0.0532\n",
      "  wins 6052 (0.86) | draws 297 (0.04) | losses 651 (0.09)\n",
      "Ep 7500/30000 | Steps 25171 | eps 0.000 | avg loss 0.0557\n",
      "  wins 6526 (0.87) | draws 310 (0.04) | losses 664 (0.09)\n",
      "Ep 8000/30000 | Steps 26852 | eps 0.000 | avg loss 0.0502\n",
      "  wins 7004 (0.88) | draws 326 (0.04) | losses 670 (0.08)\n",
      "Ep 8500/30000 | Steps 28552 | eps 0.000 | avg loss 0.0453\n",
      "  wins 7466 (0.88) | draws 355 (0.04) | losses 679 (0.08)\n",
      "Ep 9000/30000 | Steps 30243 | eps 0.000 | avg loss 0.0482\n",
      "  wins 7920 (0.88) | draws 384 (0.04) | losses 696 (0.08)\n",
      "Ep 9500/30000 | Steps 31927 | eps 0.000 | avg loss 0.0499\n",
      "  wins 8400 (0.88) | draws 396 (0.04) | losses 704 (0.07)\n",
      "Ep 10000/30000 | Steps 33612 | eps 0.000 | avg loss 0.0453\n",
      "  wins 8864 (0.89) | draws 420 (0.04) | losses 716 (0.07)\n",
      "Ep 10500/30000 | Steps 35282 | eps 0.000 | avg loss 0.0444\n",
      "  wins 9331 (0.89) | draws 437 (0.04) | losses 732 (0.07)\n",
      "Ep 11000/30000 | Steps 36980 | eps 0.000 | avg loss 0.0446\n",
      "  wins 9801 (0.89) | draws 456 (0.04) | losses 743 (0.07)\n",
      "Ep 11500/30000 | Steps 38674 | eps 0.000 | avg loss 0.0445\n",
      "  wins 10270 (0.89) | draws 474 (0.04) | losses 756 (0.07)\n",
      "Ep 12000/30000 | Steps 40373 | eps 0.000 | avg loss 0.0405\n",
      "  wins 10741 (0.90) | draws 499 (0.04) | losses 760 (0.06)\n",
      "Ep 12500/30000 | Steps 42070 | eps 0.000 | avg loss 0.0395\n",
      "  wins 11212 (0.90) | draws 512 (0.04) | losses 776 (0.06)\n",
      "Ep 13000/30000 | Steps 43800 | eps 0.000 | avg loss 0.0418\n",
      "  wins 11682 (0.90) | draws 536 (0.04) | losses 782 (0.06)\n",
      "Ep 13500/30000 | Steps 45520 | eps 0.000 | avg loss 0.0395\n",
      "  wins 12158 (0.90) | draws 552 (0.04) | losses 790 (0.06)\n",
      "Ep 14000/30000 | Steps 47235 | eps 0.000 | avg loss 0.0357\n",
      "  wins 12638 (0.90) | draws 565 (0.04) | losses 797 (0.06)\n",
      "Ep 14500/30000 | Steps 48939 | eps 0.000 | avg loss 0.0399\n",
      "  wins 13113 (0.90) | draws 583 (0.04) | losses 804 (0.06)\n",
      "Ep 15000/30000 | Steps 50657 | eps 0.000 | avg loss 0.0360\n",
      "  wins 13585 (0.91) | draws 601 (0.04) | losses 814 (0.05)\n",
      "Ep 15500/30000 | Steps 52373 | eps 0.000 | avg loss 0.0380\n",
      "  wins 14058 (0.91) | draws 615 (0.04) | losses 827 (0.05)\n",
      "Ep 16000/30000 | Steps 54056 | eps 0.000 | avg loss 0.0357\n",
      "  wins 14524 (0.91) | draws 637 (0.04) | losses 839 (0.05)\n",
      "Ep 16500/30000 | Steps 55764 | eps 0.000 | avg loss 0.0339\n",
      "  wins 14993 (0.91) | draws 662 (0.04) | losses 845 (0.05)\n",
      "Ep 17000/30000 | Steps 57453 | eps 0.000 | avg loss 0.0294\n",
      "  wins 15470 (0.91) | draws 678 (0.04) | losses 852 (0.05)\n",
      "Ep 17500/30000 | Steps 59146 | eps 0.000 | avg loss 0.0345\n",
      "  wins 15933 (0.91) | draws 707 (0.04) | losses 860 (0.05)\n",
      "Ep 18000/30000 | Steps 60818 | eps 0.000 | avg loss 0.0311\n",
      "  wins 16408 (0.91) | draws 721 (0.04) | losses 871 (0.05)\n",
      "Ep 18500/30000 | Steps 62495 | eps 0.000 | avg loss 0.0316\n",
      "  wins 16880 (0.91) | draws 736 (0.04) | losses 884 (0.05)\n",
      "Ep 19000/30000 | Steps 64206 | eps 0.000 | avg loss 0.0320\n",
      "  wins 17354 (0.91) | draws 756 (0.04) | losses 890 (0.05)\n",
      "Ep 19500/30000 | Steps 65922 | eps 0.000 | avg loss 0.0270\n",
      "  wins 17833 (0.91) | draws 774 (0.04) | losses 893 (0.05)\n",
      "Ep 20000/30000 | Steps 67594 | eps 0.000 | avg loss 0.0285\n",
      "  wins 18314 (0.92) | draws 788 (0.04) | losses 898 (0.04)\n",
      "Ep 20500/30000 | Steps 69269 | eps 0.000 | avg loss 0.0273\n",
      "  wins 18785 (0.92) | draws 806 (0.04) | losses 909 (0.04)\n",
      "Ep 21000/30000 | Steps 70954 | eps 0.000 | avg loss 0.0260\n",
      "  wins 19258 (0.92) | draws 825 (0.04) | losses 917 (0.04)\n",
      "Ep 21500/30000 | Steps 72650 | eps 0.000 | avg loss 0.0250\n",
      "  wins 19732 (0.92) | draws 844 (0.04) | losses 924 (0.04)\n",
      "Ep 22000/30000 | Steps 74346 | eps 0.000 | avg loss 0.0249\n",
      "  wins 20208 (0.92) | draws 865 (0.04) | losses 927 (0.04)\n",
      "Ep 22500/30000 | Steps 76036 | eps 0.000 | avg loss 0.0262\n",
      "  wins 20682 (0.92) | draws 885 (0.04) | losses 933 (0.04)\n",
      "Ep 23000/30000 | Steps 77717 | eps 0.000 | avg loss 0.0266\n",
      "  wins 21158 (0.92) | draws 900 (0.04) | losses 942 (0.04)\n",
      "Ep 23500/30000 | Steps 79408 | eps 0.000 | avg loss 0.0214\n",
      "  wins 21635 (0.92) | draws 919 (0.04) | losses 946 (0.04)\n",
      "Ep 24000/30000 | Steps 81126 | eps 0.000 | avg loss 0.0195\n",
      "  wins 22108 (0.92) | draws 944 (0.04) | losses 948 (0.04)\n",
      "Ep 24500/30000 | Steps 82826 | eps 0.000 | avg loss 0.0252\n",
      "  wins 22582 (0.92) | draws 966 (0.04) | losses 952 (0.04)\n",
      "Ep 25000/30000 | Steps 84524 | eps 0.000 | avg loss 0.0222\n",
      "  wins 23061 (0.92) | draws 981 (0.04) | losses 958 (0.04)\n",
      "Ep 25500/30000 | Steps 86232 | eps 0.000 | avg loss 0.0225\n",
      "  wins 23535 (0.92) | draws 1002 (0.04) | losses 963 (0.04)\n",
      "Ep 26000/30000 | Steps 87941 | eps 0.000 | avg loss 0.0230\n",
      "  wins 24017 (0.92) | draws 1019 (0.04) | losses 964 (0.04)\n",
      "Ep 26500/30000 | Steps 89650 | eps 0.000 | avg loss 0.0213\n",
      "  wins 24487 (0.92) | draws 1045 (0.04) | losses 968 (0.04)\n",
      "Ep 27000/30000 | Steps 91328 | eps 0.000 | avg loss 0.0207\n",
      "  wins 24969 (0.92) | draws 1060 (0.04) | losses 971 (0.04)\n",
      "Ep 27500/30000 | Steps 93006 | eps 0.000 | avg loss 0.0196\n",
      "  wins 25444 (0.93) | draws 1077 (0.04) | losses 979 (0.04)\n",
      "Ep 28000/30000 | Steps 94700 | eps 0.000 | avg loss 0.0210\n",
      "  wins 25922 (0.93) | draws 1089 (0.04) | losses 989 (0.04)\n",
      "Ep 28500/30000 | Steps 96409 | eps 0.000 | avg loss 0.0207\n",
      "  wins 26396 (0.93) | draws 1109 (0.04) | losses 995 (0.03)\n",
      "Ep 29000/30000 | Steps 98124 | eps 0.000 | avg loss 0.0205\n",
      "  wins 26870 (0.93) | draws 1129 (0.04) | losses 1001 (0.03)\n",
      "Ep 29500/30000 | Steps 99812 | eps 0.000 | avg loss 0.0221\n",
      "  wins 27347 (0.93) | draws 1144 (0.04) | losses 1009 (0.03)\n",
      "Ep 30000/30000 | Steps 101497 | eps 0.000 | avg loss 0.0230\n",
      "  wins 27818 (0.93) | draws 1166 (0.04) | losses 1016 (0.03)\n",
      "Training finished. Model saved to dqn_tictactoe.pth\n",
      "Eval over 1000: wins 992, draws 8, losses 0\n"
     ]
    }
   ],
   "source": [
    "# dqn_tictactoe.py\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# --------- Hyperparams ---------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "NUM_EPISODES = 30000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LR = 1e-4\n",
    "REPLAY_SIZE = 50000\n",
    "MIN_REPLAY_SIZE = 500\n",
    "TARGET_UPDATE_EVERY = 500  # steps\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.\n",
    "EPS_DECAY = 8000  # linear decay steps\n",
    "MAX_STEPS_PER_EPISODE = 9\n",
    "PRINT_EVERY = 500\n",
    "SAVE_PATH = \"dqn_tictactoe.pth\"\n",
    "\n",
    "Transition = namedtuple('Transition', ('s', 'a', 'r', 's2', 'done', 'mask_next'))\n",
    "\n",
    "# --------- Environment ---------\n",
    "class TicTacToeEnv:\n",
    "    \"\"\"\n",
    "    Agent plays '1' (X). Opponent plays '-1' (O).\n",
    "    State: numpy array shape (9,) with values in {-1,0,1}.\n",
    "    step(action): agent plays action (0..8), then opponent plays (random).\n",
    "    returns: next_state, reward, done, info\n",
    "    reward: +1 win for agent, -1 loss, 0 otherwise (draw -> 0)\n",
    "    \"\"\"\n",
    "    def __init__(self, opponent_policy=\"random\"):\n",
    "        self.board = np.zeros(9, dtype=int)\n",
    "        self.current_player = 1  # agent starts by default\n",
    "        self.opponent_policy = opponent_policy\n",
    "\n",
    "    def reset(self, agent_starts=True):\n",
    "        self.board[:] = 0\n",
    "        self.current_player = 1 if agent_starts else -1\n",
    "        # If opponent starts, let them play one move then return state to agent\n",
    "        if not agent_starts:\n",
    "            self._opponent_move()\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        return self.board.copy()\n",
    "\n",
    "    def _available_actions(self):\n",
    "        return np.where(self.board == 0)[0]\n",
    "\n",
    "    def _check_winner(self, b):\n",
    "        lines = [\n",
    "            (0,1,2),(3,4,5),(6,7,8),\n",
    "            (0,3,6),(1,4,7),(2,5,8),\n",
    "            (0,4,8),(2,4,6)\n",
    "        ]\n",
    "        for (i,j,k) in lines:\n",
    "            s = b[i] + b[j] + b[k]\n",
    "            if s == 3:\n",
    "                return 1\n",
    "            if s == -3:\n",
    "                return -1\n",
    "        if np.all(b != 0):\n",
    "            return 0  # draw\n",
    "        return None  # game not finished\n",
    "\n",
    "    def _opponent_move(self):\n",
    "        avail = self._available_actions()\n",
    "        if len(avail) == 0:\n",
    "            return\n",
    "        if self.opponent_policy == \"random\":\n",
    "            a = np.random.choice(avail)\n",
    "        else:\n",
    "            a = np.random.choice(avail)\n",
    "        self.board[a] = -1\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Agent action -> apply -> check -> opponent -> check.\n",
    "        Returns: next_state, reward, done, info\n",
    "        \"\"\"\n",
    "        info = {}\n",
    "        # illegal move\n",
    "        if self.board[action] != 0:\n",
    "            # punish illegal moves harshly\n",
    "            return self._get_state(), -1.0, True, {\"illegal\": True}\n",
    "\n",
    "        # agent plays\n",
    "        self.board[action] = 1\n",
    "        winner = self._check_winner(self.board)\n",
    "        if winner is not None:\n",
    "            if winner == 1:\n",
    "                return self._get_state(), 1.0, True, info\n",
    "            elif winner == 0:  # draw\n",
    "                return self._get_state(), 0.0, True, info\n",
    "\n",
    "        # opponent plays\n",
    "        self._opponent_move()\n",
    "        winner = self._check_winner(self.board)\n",
    "        if winner is not None:\n",
    "            if winner == -1:\n",
    "                return self._get_state(), -1.0, True, info\n",
    "            elif winner == 0:\n",
    "                return self._get_state(), 0.0, True, info\n",
    "\n",
    "        # game continues\n",
    "        return self._get_state(), 0.0, False, info\n",
    "\n",
    "# --------- Replay Buffer ---------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return Transition(*zip(*batch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# --------- DQN Model ---------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim=9, output_dim=9):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# --------- Helpers ---------\n",
    "def state_to_tensor(state):\n",
    "    # state: numpy array shape (9,) with -1,0,1\n",
    "    # convert to float tensor\n",
    "    return torch.tensor(state, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "def mask_invalid(q_values, state):\n",
    "    # q_values: tensor shape (..., 9)\n",
    "    # state: numpy array shape (9,) where 0 means available\n",
    "    mask = (state != 0)  # True where not available\n",
    "    q_values = q_values.clone()\n",
    "    q_values[..., mask] = -1e9\n",
    "    return q_values\n",
    "\n",
    "# --------- Training loop ---------\n",
    "def train():\n",
    "    env = TicTacToeEnv(opponent_policy=\"random\")\n",
    "    policy_net = DQN().to(DEVICE)\n",
    "    target_net = DQN().to(DEVICE)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "    replay = ReplayBuffer(REPLAY_SIZE)\n",
    "\n",
    "    # populate initial replay with random games\n",
    "    steps_done = 0\n",
    "    while len(replay) < MIN_REPLAY_SIZE:\n",
    "        state = env.reset(agent_starts=True)\n",
    "        done = False\n",
    "        while not done:\n",
    "            avail = env._available_actions()\n",
    "            a = np.random.choice(avail)\n",
    "            s2, r, done, _ = env.step(a)\n",
    "            mask_next = (s2 != 0)\n",
    "            replay.push(state.copy(), a, r, s2.copy(), done, mask_next.copy())\n",
    "            state = s2\n",
    "    print(f\"Replay buffer initialized with {len(replay)} transitions.\")\n",
    "\n",
    "    eps = EPS_START\n",
    "    eps_decay_per_step = (EPS_START - EPS_END) / EPS_DECAY\n",
    "\n",
    "    losses = []\n",
    "    total_steps = 0\n",
    "    win_count = 0\n",
    "    draw_count = 0\n",
    "    loss_count = 0\n",
    "\n",
    "    for ep in range(1, NUM_EPISODES + 1):\n",
    "        # alternate who starts occasionally to diversify\n",
    "        agent_starts = True if random.random() < 0.5 else False\n",
    "        state = env.reset(agent_starts=agent_starts)\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done and steps < MAX_STEPS_PER_EPISODE:\n",
    "            steps += 1\n",
    "            total_steps += 1\n",
    "            # epsilon-greedy action selection with mask\n",
    "            state_tensor = state_to_tensor(state)\n",
    "            with torch.no_grad():\n",
    "                qvals = policy_net(state_tensor)  # shape (1,9)\n",
    "                qvals_masked = mask_invalid(qvals, state)\n",
    "            if random.random() < eps:\n",
    "                action = int(np.random.choice(np.where(state == 0)[0]))\n",
    "            else:\n",
    "                action = int(torch.argmax(qvals_masked).item())\n",
    "\n",
    "            # step environment (this will apply opponent move inside)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # store transition\n",
    "            mask_next = (next_state != 0)\n",
    "            replay.push(state.copy(), action, reward, next_state.copy(), done, mask_next.copy())\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # sample and train\n",
    "            if len(replay) >= MIN_REPLAY_SIZE:\n",
    "                batch = replay.sample(BATCH_SIZE)\n",
    "                s_batch = torch.tensor(np.stack(batch.s), dtype=torch.float32, device=DEVICE)  # (B,9)\n",
    "                a_batch = torch.tensor(batch.a, dtype=torch.long, device=DEVICE).unsqueeze(1)  # (B,1)\n",
    "                r_batch = torch.tensor(batch.r, dtype=torch.float32, device=DEVICE).unsqueeze(1)  # (B,1)\n",
    "                s2_batch = torch.tensor(np.stack(batch.s2), dtype=torch.float32, device=DEVICE)  # (B,9)\n",
    "                done_batch = torch.tensor(batch.done, dtype=torch.bool, device=DEVICE).unsqueeze(1)\n",
    "                mask_next_batch = torch.tensor(np.stack(batch.mask_next), dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "                # current Q(s,a)\n",
    "                q_values = policy_net(s_batch)  # (B,9)\n",
    "                q_s_a = q_values.gather(1, a_batch)  # (B,1)\n",
    "\n",
    "                # compute target\n",
    "                with torch.no_grad():\n",
    "                    q_next_target = target_net(s2_batch)  # (B,9)\n",
    "                    # mask invalid next actions (where mask_next_batch == True -> occupied => invalid)\n",
    "                    invalid_mask = mask_next_batch  # True where occupied\n",
    "                    q_next_target[invalid_mask] = -1e9\n",
    "                    max_q_next, _ = q_next_target.max(dim=1, keepdim=True)  # (B,1)\n",
    "                    y = r_batch + (GAMMA * max_q_next * (~done_batch))\n",
    "                    # if done -> y = r (done mask handles it)\n",
    "\n",
    "                loss = nn.MSELoss()(q_s_a, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "\n",
    "            # update target network\n",
    "            if total_steps % TARGET_UPDATE_EVERY == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            # decay epsilon\n",
    "            if eps > EPS_END:\n",
    "                eps -= eps_decay_per_step\n",
    "                if eps < EPS_END:\n",
    "                    eps = EPS_END\n",
    "\n",
    "        # stats\n",
    "        if reward == 1.0:\n",
    "            win_count += 1\n",
    "        elif reward == -1.0:\n",
    "            loss_count += 1\n",
    "        else:\n",
    "            draw_count += 1\n",
    "\n",
    "        if ep % PRINT_EVERY == 0:\n",
    "            total = win_count + loss_count + draw_count\n",
    "            print(f\"Ep {ep}/{NUM_EPISODES} | Steps {total_steps} | eps {eps:.3f} | avg loss {np.mean(losses[-200:]) if losses else 0:.4f}\")\n",
    "            print(f\"  wins {win_count} ({win_count/total:.2f}) | draws {draw_count} ({draw_count/total:.2f}) | losses {loss_count} ({loss_count/total:.2f})\")\n",
    "\n",
    "    # save model\n",
    "    torch.save(policy_net.state_dict(), SAVE_PATH)\n",
    "    print(\"Training finished. Model saved to\", SAVE_PATH)\n",
    "    return policy_net\n",
    "\n",
    "# --------- Evaluate ---------\n",
    "def evaluate(net, episodes=1000):\n",
    "    env = TicTacToeEnv(opponent_policy=\"random\")\n",
    "    net.eval()\n",
    "    wins = draws = losses = 0\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset(agent_starts=True)\n",
    "        done = False\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                q = net(state_to_tensor(state))\n",
    "                q_masked = mask_invalid(q, state)\n",
    "                action = int(torch.argmax(q_masked).item())\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "        if reward == 1:\n",
    "            wins += 1\n",
    "        elif reward == -1:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "    print(f\"Eval over {episodes}: wins {wins}, draws {draws}, losses {losses}\")\n",
    "    return wins, draws, losses\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trained = train()\n",
    "    evaluate(trained, episodes=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
