{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e9ef0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ConnectFourEnv():\n",
    "    def __init__(self) -> None:\n",
    "        self.rows = 6\n",
    "        self.cols = 7\n",
    "        # 1D board of size 42\n",
    "        self.board = np.zeros(shape=(self.rows * self.cols,), dtype=int)\n",
    "        self.computer = 1\n",
    "        self.opponent = -1\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the board and handles who moves first.\"\"\"\n",
    "        self.board[:] = 0\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        \n",
    "        # Randomly decide who goes first\n",
    "        self.mover = np.random.choice([self.computer, self.opponent])\n",
    "        \n",
    "        # If opponent starts, they make a random move immediately\n",
    "        if self.mover == self.opponent:\n",
    "            action = self.random_action()\n",
    "            self.apply_action(action, self.opponent)\n",
    "            \n",
    "        return self.board.copy()\n",
    "\n",
    "    def available_actions_idx(self):\n",
    "        \"\"\"Returns a list of column indices (0-6) that are not full.\"\"\"\n",
    "        # Reshape to 2D to easily check the top row (row 0)\n",
    "        board_2d = self.board.reshape(self.rows, self.cols)\n",
    "        # If the top row (0) at column c is 0, the column is valid\n",
    "        return [c for c in range(self.cols) if board_2d[0, c] == 0]\n",
    "\n",
    "    def random_action(self):\n",
    "        \"\"\"Returns a random valid column.\"\"\"\n",
    "        possible_cols = self.available_actions_idx()\n",
    "        if not possible_cols:\n",
    "            return None # Draw/Full\n",
    "        return np.random.choice(possible_cols)\n",
    "\n",
    "    def apply_action(self, col_idx, player):\n",
    "        \"\"\"\n",
    "        Simulates gravity: places the player's piece in the \n",
    "        lowest available row in the given column.\n",
    "        \"\"\"\n",
    "        board_2d = self.board.reshape(self.rows, self.cols)\n",
    "        \n",
    "        # Find the lowest empty row in this column\n",
    "        # We scan from bottom (row 5) to top (row 0)\n",
    "        for r in range(self.rows - 1, -1, -1):\n",
    "            if board_2d[r, col_idx] == 0:\n",
    "                board_2d[r, col_idx] = player\n",
    "                break\n",
    "        \n",
    "        # Flatten back to 1D to update self.board\n",
    "        self.board = board_2d.flatten()\n",
    "\n",
    "    def check_win(self, player):\n",
    "        \"\"\"Checks horizontal, vertical, and diagonal lines for 4 connected.\"\"\"\n",
    "        board_2d = self.board.reshape(self.rows, self.cols)\n",
    "\n",
    "        # 1. Horizontal\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols - 3):\n",
    "                if np.all(board_2d[r, c:c+4] == player):\n",
    "                    return True\n",
    "\n",
    "        # 2. Vertical\n",
    "        for r in range(self.rows - 3):\n",
    "            for c in range(self.cols):\n",
    "                if np.all(board_2d[r:r+4, c] == player):\n",
    "                    return True\n",
    "\n",
    "        # 3. Diagonal (\\)\n",
    "        for r in range(self.rows - 3):\n",
    "            for c in range(self.cols - 3):\n",
    "                if np.all([board_2d[r+i, c+i] == player for i in range(4)]):\n",
    "                    return True\n",
    "\n",
    "        # 4. Anti-Diagonal (/)\n",
    "        for r in range(3, self.rows):\n",
    "            for c in range(self.cols - 3):\n",
    "                if np.all([board_2d[r-i, c+i] == player for i in range(4)]):\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def step(self, action, opponent_model=None): # <--- Check this argument\n",
    "        # 1. Check Agent Valid Move\n",
    "        if action not in self.available_actions_idx():\n",
    "             return self.board.copy(), -10, True, {\"result\": \"Error\"}\n",
    "        \n",
    "        # 2. Agent Move\n",
    "        self.apply_action(action, self.computer)\n",
    "        if self.check_win(self.computer):\n",
    "            return self.board.copy(), 1, True, {\"result\": \"Win\"}\n",
    "        if len(self.available_actions_idx()) == 0:\n",
    "            return self.board.copy(), 0, True, {\"result\": \"Draw\"}\n",
    "\n",
    "        # 3. Opponent Move\n",
    "        if opponent_model is None:\n",
    "            # Default: Random\n",
    "            opp_action = self.random_action()\n",
    "        else:\n",
    "            # Advanced: The Clone\n",
    "            opp_action = self.get_opponent_action(opponent_model) # <--- Make sure this is called\n",
    "            \n",
    "        self.apply_action(opp_action, self.opponent)\n",
    "\n",
    "        if self.check_win(self.opponent):\n",
    "            return self.board.copy(), -1, True, {\"result\": \"Loss\"}\n",
    "        if len(self.available_actions_idx()) == 0:\n",
    "            return self.board.copy(), 0, True, {\"result\": \"Draw\"}\n",
    "\n",
    "        return self.board.copy(), 0, False, {}\n",
    "\n",
    "    def get_opponent_action(self, model):\n",
    "        board_for_opp = self.board * -1 \n",
    "        state_t = torch.tensor(board_for_opp, dtype=torch.float32).unsqueeze(0).view(1, 1, 6, 7) # Ensure shape\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_vals = model(state_t)\n",
    "            valid_moves = self.available_actions_idx()\n",
    "            mask = torch.full_like(q_vals, -float('inf'))\n",
    "            mask[0, valid_moves] = q_vals[0, valid_moves]\n",
    "            action = mask.max(1)[1].item()\n",
    "        return action\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Visualizes the board.\"\"\"\n",
    "        board_2d = self.board.reshape(self.rows, self.cols)\n",
    "        symbols = {0: '.', 1: 'X', -1: 'O'}\n",
    "        print(\"\\nBoard State:\")\n",
    "        for row in board_2d:\n",
    "            print(\" \".join([symbols[x] for x in row]))\n",
    "        print(\"-\" * 13)\n",
    "        print(\"0 1 2 3 4 5 6\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e44fa562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNConnectFour(nn.Module):\n",
    "    def __init__(self, output_dim=7):\n",
    "        super(QNConnectFour, self).__init__()\n",
    "        \n",
    "        # --- Convolutional Block ---\n",
    "        # We treat the board as an image: 1 channel (the values -1, 0, 1), 6 rows, 7 cols\n",
    "        \n",
    "        # Conv1: Expands features. looks for small local patterns\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64) # Normalization helps faster convergence\n",
    "        \n",
    "        # Conv2: Goes deeper\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Conv3: Refines features\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # --- Fully Connected Block ---\n",
    "        # Flatten: 128 channels * 6 rows * 7 cols = 5376\n",
    "        self.fc1 = nn.Linear(128 * 6 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim) # Output is 7 (one Q-value per column)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Reshape Input\n",
    "        # The environment gives us a flat vector (Batch, 42).\n",
    "        # We must reshape it to (Batch, 1, 6, 7) for the CNN.\n",
    "        x = x.view(-1, 1, 6, 7) \n",
    "        \n",
    "        # 2. Convolutions + Activations\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # 3. Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # 4. Dense Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # 5. Output (No activation here, raw Q-values)\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7fdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf42ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # 1. Initialize Networks\n",
    "        # Policy Net: The one we train\n",
    "        self.policy_net = QNConnectFour(output_dim).to(self.device)\n",
    "        # Target Net: A stable copy to calculate future rewards (stabilizes training)\n",
    "        self.target_net = QNConnectFour(output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval() # Set to evaluation mode\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.0001)\n",
    "        self.memory = ReplayMemory(10000)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.GAMMA = 0.99  # Discount factor (cares about long term)\n",
    "        self.EPS_START = 1.0\n",
    "        self.EPS_END = 0.05\n",
    "        self.EPS_DECAY = 1000 # How fast exploration decays\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, state, valid_moves):\n",
    "        \"\"\"\n",
    "        Epsilon-Greedy strategy with invalid move masking.\n",
    "        \"\"\"\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "\n",
    "        # EXPLORATION: Pick random valid move\n",
    "        if sample < eps_threshold:\n",
    "            return torch.tensor([[random.choice(valid_moves)]], device=self.device, dtype=torch.long)\n",
    "        \n",
    "        # EXPLOITATION: Pick best move from Network\n",
    "        with torch.no_grad():\n",
    "            # Get Q-values from network\n",
    "            q_values = self.policy_net(state.to(self.device))\n",
    "            \n",
    "            # Mask invalid moves: Set their Q-value to negative infinity so they aren't picked\n",
    "            # Create a mask of -inf\n",
    "            mask = torch.full_like(q_values, -float('inf'))\n",
    "            # Set valid indices to the actual q_values\n",
    "            mask[0, valid_moves] = q_values[0, valid_moves]\n",
    "            \n",
    "            # Return index of max value\n",
    "            return mask.max(1)[1].view(1, 1)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Convert batch data to tensors\n",
    "        state_batch = torch.cat(batch.state).to(self.device)\n",
    "        action_batch = torch.cat(batch.action).to(self.device)\n",
    "        reward_batch = torch.cat(batch.reward).to(self.device)\n",
    "        next_state_batch = torch.cat(batch.next_state).to(self.device)\n",
    "        done_batch = torch.cat(batch.done).to(self.device)\n",
    "\n",
    "        # 1. Compute Q(s_t, a) - The Q-values we estimated\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 2. Compute V(s_{t+1}) for all next states using Target Net\n",
    "        next_state_values = self.target_net(next_state_batch).max(1)[0].detach()\n",
    "        \n",
    "        # 3. Compute the expected Q values (Bellman Equation)\n",
    "        # If done, expected_q is just reward. If not, reward + gamma * best_future_q\n",
    "        expected_state_action_values = reward_batch + (self.GAMMA * next_state_values * (1 - done_batch))\n",
    "\n",
    "        # 4. Compute Huber Loss (Smooth L1)\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 5. Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients (common in RL)\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfad320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import torch\n",
    "\n",
    "def training(env, agent, num_episodes=1000, target_update_freq=10):\n",
    "\n",
    "    # Initialize\n",
    "    opponent_net = None \n",
    "    win_history = []\n",
    "    loss_history = [] # <--- NEW: To store average loss per episode\n",
    "    win_rate_threshold = 0.85 \n",
    "\n",
    "    print(\"Starting Training...\")\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        state_np = env.reset()\n",
    "        state = torch.tensor(state_np, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # <--- NEW: Variables to track loss within this specific episode\n",
    "        episode_loss_sum = 0\n",
    "        episode_opt_count = 0 \n",
    "        \n",
    "        while not done:\n",
    "            # 1. Select Action\n",
    "            valid_moves = env.available_actions_idx()\n",
    "            action_tensor = agent.select_action(state, valid_moves)\n",
    "            action = action_tensor.item() \n",
    "            \n",
    "            # 2. Step Environment\n",
    "            next_state_np, reward, done, info = env.step(action, opponent_model=opponent_net)\n",
    "            \n",
    "            # 3. Process Reward & Next State\n",
    "            reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "            next_state = torch.tensor(next_state_np, dtype=torch.float32).unsqueeze(0)\n",
    "            done_tensor = torch.tensor([float(done)], dtype=torch.float32)\n",
    "\n",
    "            # 4. Store in Memory\n",
    "            agent.memory.push(state, action_tensor, reward_tensor, next_state, done_tensor)\n",
    "\n",
    "            # 5. Move to next state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # 6. Perform one step of optimization\n",
    "            loss = agent.optimize_model()\n",
    "            \n",
    "            # <--- NEW: Accumulate loss\n",
    "            if loss is not None:\n",
    "                episode_loss_sum += loss.item() # .item() is crucial to save memory!\n",
    "                episode_opt_count += 1\n",
    "\n",
    "        # --- TRACKING LOSS --- # <--- NEW\n",
    "        if episode_opt_count > 0:\n",
    "            avg_ep_loss = episode_loss_sum / episode_opt_count\n",
    "            loss_history.append(avg_ep_loss)\n",
    "        else:\n",
    "            loss_history.append(0)\n",
    "\n",
    "        # --- TRACKING WINS ---\n",
    "        if info['result'] == 'Win':\n",
    "            win_history.append(1)\n",
    "        else:\n",
    "            win_history.append(0)\n",
    "            \n",
    "        # Keep only last 100 games\n",
    "        if len(win_history) > 100: win_history.pop(0)\n",
    "        if len(loss_history) > 100: loss_history.pop(0) # Keep loss history same size\n",
    "            \n",
    "        # --- THE UPDATE CHECK ---\n",
    "        if i_episode % 50 == 0 and len(win_history) == 100:\n",
    "            win_rate = sum(win_history) / 100\n",
    "            # <--- NEW: Calculate average loss over the last 100 episodes\n",
    "            avg_loss_stat = sum(loss_history) / len(loss_history)\n",
    "            \n",
    "            # Calculate epsilon for display\n",
    "            curr_eps = agent.EPS_END + (agent.EPS_START - agent.EPS_END) * math.exp(-1. * agent.steps_done / agent.EPS_DECAY)\n",
    "            \n",
    "            print(f\"Episode {i_episode} | Win Rate: {win_rate:.2f} | Avg Loss: {avg_loss_stat:.6f} | Epsilon: {curr_eps:.4f}\")\n",
    "            \n",
    "            if win_rate > win_rate_threshold:\n",
    "                print(f\"ðŸš€ PROMOTION! Agent (Win Rate {win_rate:.2f}) is now the Opponent.\")\n",
    "                \n",
    "                opponent_net = copy.deepcopy(agent.policy_net)\n",
    "                opponent_net.eval()\n",
    "                \n",
    "                win_history = [] \n",
    "                loss_history = [] # Optional: Reset loss history too if you want fresh stats\n",
    "                \n",
    "                agent.steps_done = int(agent.EPS_DECAY * 2)\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8f593a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Episode 100 | Win Rate: 0.57 | Avg Loss: 0.003333 | Epsilon: 0.4123\n",
      "Episode 150 | Win Rate: 0.62 | Avg Loss: 0.002755 | Epsilon: 0.2845\n",
      "Episode 200 | Win Rate: 0.70 | Avg Loss: 0.002514 | Epsilon: 0.2009\n",
      "Episode 250 | Win Rate: 0.76 | Avg Loss: 0.002468 | Epsilon: 0.1484\n",
      "Episode 300 | Win Rate: 0.76 | Avg Loss: 0.002277 | Epsilon: 0.1170\n",
      "Episode 350 | Win Rate: 0.77 | Avg Loss: 0.001937 | Epsilon: 0.0949\n",
      "Episode 400 | Win Rate: 0.81 | Avg Loss: 0.001880 | Epsilon: 0.0799\n",
      "Episode 450 | Win Rate: 0.78 | Avg Loss: 0.001874 | Epsilon: 0.0704\n",
      "Episode 500 | Win Rate: 0.78 | Avg Loss: 0.001919 | Epsilon: 0.0643\n",
      "Episode 550 | Win Rate: 0.82 | Avg Loss: 0.001963 | Epsilon: 0.0599\n",
      "Episode 600 | Win Rate: 0.83 | Avg Loss: 0.001720 | Epsilon: 0.0567\n",
      "Episode 650 | Win Rate: 0.80 | Avg Loss: 0.001698 | Epsilon: 0.0547\n",
      "Episode 700 | Win Rate: 0.77 | Avg Loss: 0.001849 | Epsilon: 0.0532\n",
      "Episode 750 | Win Rate: 0.76 | Avg Loss: 0.001733 | Epsilon: 0.0522\n",
      "Episode 800 | Win Rate: 0.77 | Avg Loss: 0.001686 | Epsilon: 0.0515\n",
      "Episode 850 | Win Rate: 0.74 | Avg Loss: 0.001893 | Epsilon: 0.0511\n",
      "Episode 900 | Win Rate: 0.77 | Avg Loss: 0.001742 | Epsilon: 0.0507\n",
      "Episode 950 | Win Rate: 0.80 | Avg Loss: 0.001701 | Epsilon: 0.0505\n",
      "Episode 1000 | Win Rate: 0.77 | Avg Loss: 0.001674 | Epsilon: 0.0503\n",
      "Episode 1050 | Win Rate: 0.76 | Avg Loss: 0.001468 | Epsilon: 0.0502\n",
      "Episode 1100 | Win Rate: 0.79 | Avg Loss: 0.001848 | Epsilon: 0.0501\n",
      "Episode 1150 | Win Rate: 0.81 | Avg Loss: 0.001869 | Epsilon: 0.0501\n",
      "Episode 1200 | Win Rate: 0.79 | Avg Loss: 0.001685 | Epsilon: 0.0501\n",
      "Episode 1250 | Win Rate: 0.82 | Avg Loss: 0.001664 | Epsilon: 0.0500\n",
      "Episode 1300 | Win Rate: 0.81 | Avg Loss: 0.001461 | Epsilon: 0.0500\n",
      "Episode 1350 | Win Rate: 0.86 | Avg Loss: 0.001612 | Epsilon: 0.0500\n",
      "ðŸš€ PROMOTION! Agent (Win Rate 0.86) is now the Opponent.\n",
      "Episode 1450 | Win Rate: 0.44 | Avg Loss: 0.001959 | Epsilon: 0.1191\n",
      "Episode 1500 | Win Rate: 0.47 | Avg Loss: 0.002364 | Epsilon: 0.1007\n",
      "Episode 1550 | Win Rate: 0.49 | Avg Loss: 0.002621 | Epsilon: 0.0881\n",
      "Episode 1600 | Win Rate: 0.49 | Avg Loss: 0.002778 | Epsilon: 0.0772\n",
      "Episode 1650 | Win Rate: 0.53 | Avg Loss: 0.002746 | Epsilon: 0.0695\n",
      "Episode 1700 | Win Rate: 0.57 | Avg Loss: 0.002473 | Epsilon: 0.0636\n",
      "Episode 1750 | Win Rate: 0.60 | Avg Loss: 0.002097 | Epsilon: 0.0597\n",
      "Episode 1800 | Win Rate: 0.65 | Avg Loss: 0.001734 | Epsilon: 0.0570\n",
      "Episode 1850 | Win Rate: 0.62 | Avg Loss: 0.001781 | Epsilon: 0.0548\n",
      "Episode 1900 | Win Rate: 0.67 | Avg Loss: 0.001845 | Epsilon: 0.0535\n",
      "Episode 1950 | Win Rate: 0.76 | Avg Loss: 0.001681 | Epsilon: 0.0525\n",
      "Episode 2000 | Win Rate: 0.74 | Avg Loss: 0.001667 | Epsilon: 0.0518\n",
      "Episode 2050 | Win Rate: 0.69 | Avg Loss: 0.001654 | Epsilon: 0.0513\n",
      "Episode 2100 | Win Rate: 0.66 | Avg Loss: 0.001636 | Epsilon: 0.0509\n",
      "Episode 2150 | Win Rate: 0.76 | Avg Loss: 0.001453 | Epsilon: 0.0506\n",
      "Episode 2200 | Win Rate: 0.72 | Avg Loss: 0.001453 | Epsilon: 0.0504\n",
      "Episode 2250 | Win Rate: 0.72 | Avg Loss: 0.001628 | Epsilon: 0.0503\n",
      "Episode 2300 | Win Rate: 0.77 | Avg Loss: 0.001489 | Epsilon: 0.0502\n",
      "Episode 2350 | Win Rate: 0.75 | Avg Loss: 0.001379 | Epsilon: 0.0502\n",
      "Episode 2400 | Win Rate: 0.75 | Avg Loss: 0.001449 | Epsilon: 0.0501\n",
      "Episode 2450 | Win Rate: 0.70 | Avg Loss: 0.001418 | Epsilon: 0.0501\n",
      "Episode 2500 | Win Rate: 0.68 | Avg Loss: 0.001416 | Epsilon: 0.0501\n",
      "Episode 2550 | Win Rate: 0.68 | Avg Loss: 0.001409 | Epsilon: 0.0500\n",
      "Episode 2600 | Win Rate: 0.70 | Avg Loss: 0.001268 | Epsilon: 0.0500\n",
      "Episode 2650 | Win Rate: 0.78 | Avg Loss: 0.001226 | Epsilon: 0.0500\n",
      "Episode 2700 | Win Rate: 0.84 | Avg Loss: 0.001080 | Epsilon: 0.0500\n",
      "Episode 2750 | Win Rate: 0.79 | Avg Loss: 0.000977 | Epsilon: 0.0500\n",
      "Episode 2800 | Win Rate: 0.71 | Avg Loss: 0.001080 | Epsilon: 0.0500\n",
      "Episode 2850 | Win Rate: 0.71 | Avg Loss: 0.001127 | Epsilon: 0.0500\n",
      "Episode 2900 | Win Rate: 0.75 | Avg Loss: 0.001061 | Epsilon: 0.0500\n",
      "Episode 2950 | Win Rate: 0.77 | Avg Loss: 0.000974 | Epsilon: 0.0500\n",
      "Episode 3000 | Win Rate: 0.77 | Avg Loss: 0.000891 | Epsilon: 0.0500\n",
      "Episode 3050 | Win Rate: 0.80 | Avg Loss: 0.000808 | Epsilon: 0.0500\n",
      "Episode 3100 | Win Rate: 0.77 | Avg Loss: 0.000916 | Epsilon: 0.0500\n",
      "Episode 3150 | Win Rate: 0.65 | Avg Loss: 0.001193 | Epsilon: 0.0500\n",
      "Episode 3200 | Win Rate: 0.71 | Avg Loss: 0.001127 | Epsilon: 0.0500\n",
      "Episode 3250 | Win Rate: 0.78 | Avg Loss: 0.001009 | Epsilon: 0.0500\n",
      "Episode 3300 | Win Rate: 0.70 | Avg Loss: 0.001087 | Epsilon: 0.0500\n",
      "Episode 3350 | Win Rate: 0.73 | Avg Loss: 0.001003 | Epsilon: 0.0500\n",
      "Episode 3400 | Win Rate: 0.79 | Avg Loss: 0.000966 | Epsilon: 0.0500\n",
      "Episode 3450 | Win Rate: 0.75 | Avg Loss: 0.000949 | Epsilon: 0.0500\n",
      "Episode 3500 | Win Rate: 0.72 | Avg Loss: 0.000811 | Epsilon: 0.0500\n",
      "Episode 3550 | Win Rate: 0.79 | Avg Loss: 0.000834 | Epsilon: 0.0500\n",
      "Episode 3600 | Win Rate: 0.81 | Avg Loss: 0.001094 | Epsilon: 0.0500\n",
      "Episode 3650 | Win Rate: 0.78 | Avg Loss: 0.001132 | Epsilon: 0.0500\n",
      "Episode 3700 | Win Rate: 0.75 | Avg Loss: 0.001093 | Epsilon: 0.0500\n",
      "Episode 3750 | Win Rate: 0.77 | Avg Loss: 0.000938 | Epsilon: 0.0500\n",
      "Episode 3800 | Win Rate: 0.80 | Avg Loss: 0.000635 | Epsilon: 0.0500\n",
      "Episode 3850 | Win Rate: 0.77 | Avg Loss: 0.000663 | Epsilon: 0.0500\n",
      "Episode 3900 | Win Rate: 0.73 | Avg Loss: 0.000783 | Epsilon: 0.0500\n",
      "Episode 3950 | Win Rate: 0.76 | Avg Loss: 0.001082 | Epsilon: 0.0500\n",
      "Episode 4000 | Win Rate: 0.82 | Avg Loss: 0.001239 | Epsilon: 0.0500\n",
      "Episode 4050 | Win Rate: 0.84 | Avg Loss: 0.000999 | Epsilon: 0.0500\n",
      "Episode 4100 | Win Rate: 0.84 | Avg Loss: 0.000874 | Epsilon: 0.0500\n",
      "Episode 4150 | Win Rate: 0.72 | Avg Loss: 0.000854 | Epsilon: 0.0500\n",
      "Episode 4200 | Win Rate: 0.66 | Avg Loss: 0.000874 | Epsilon: 0.0500\n",
      "Episode 4250 | Win Rate: 0.72 | Avg Loss: 0.000934 | Epsilon: 0.0500\n",
      "Episode 4300 | Win Rate: 0.69 | Avg Loss: 0.000891 | Epsilon: 0.0500\n",
      "Episode 4350 | Win Rate: 0.65 | Avg Loss: 0.000828 | Epsilon: 0.0500\n",
      "Episode 4400 | Win Rate: 0.72 | Avg Loss: 0.000863 | Epsilon: 0.0500\n",
      "Episode 4450 | Win Rate: 0.81 | Avg Loss: 0.000848 | Epsilon: 0.0500\n",
      "Episode 4500 | Win Rate: 0.83 | Avg Loss: 0.000803 | Epsilon: 0.0500\n",
      "Episode 4550 | Win Rate: 0.72 | Avg Loss: 0.000776 | Epsilon: 0.0500\n",
      "Episode 4600 | Win Rate: 0.68 | Avg Loss: 0.000816 | Epsilon: 0.0500\n",
      "Episode 4650 | Win Rate: 0.71 | Avg Loss: 0.000898 | Epsilon: 0.0500\n",
      "Episode 4700 | Win Rate: 0.69 | Avg Loss: 0.001016 | Epsilon: 0.0500\n",
      "Episode 4750 | Win Rate: 0.72 | Avg Loss: 0.001222 | Epsilon: 0.0500\n",
      "Episode 4800 | Win Rate: 0.81 | Avg Loss: 0.001070 | Epsilon: 0.0500\n",
      "Episode 4850 | Win Rate: 0.79 | Avg Loss: 0.000864 | Epsilon: 0.0500\n",
      "Episode 4900 | Win Rate: 0.73 | Avg Loss: 0.000966 | Epsilon: 0.0500\n",
      "Episode 4950 | Win Rate: 0.77 | Avg Loss: 0.001052 | Epsilon: 0.0500\n",
      "Episode 5000 | Win Rate: 0.87 | Avg Loss: 0.000915 | Epsilon: 0.0500\n",
      "ðŸš€ PROMOTION! Agent (Win Rate 0.87) is now the Opponent.\n",
      "Episode 5100 | Win Rate: 0.42 | Avg Loss: 0.001478 | Epsilon: 0.0937\n",
      "Episode 5150 | Win Rate: 0.38 | Avg Loss: 0.001651 | Epsilon: 0.0758\n",
      "Episode 5200 | Win Rate: 0.44 | Avg Loss: 0.001902 | Epsilon: 0.0654\n",
      "Episode 5250 | Win Rate: 0.53 | Avg Loss: 0.002100 | Epsilon: 0.0589\n",
      "Episode 5300 | Win Rate: 0.52 | Avg Loss: 0.002085 | Epsilon: 0.0554\n",
      "Episode 5350 | Win Rate: 0.49 | Avg Loss: 0.002296 | Epsilon: 0.0533\n",
      "Episode 5400 | Win Rate: 0.52 | Avg Loss: 0.002157 | Epsilon: 0.0518\n",
      "Episode 5450 | Win Rate: 0.58 | Avg Loss: 0.001885 | Epsilon: 0.0511\n",
      "Episode 5500 | Win Rate: 0.56 | Avg Loss: 0.001875 | Epsilon: 0.0506\n",
      "Episode 5550 | Win Rate: 0.47 | Avg Loss: 0.001963 | Epsilon: 0.0504\n",
      "Episode 5600 | Win Rate: 0.49 | Avg Loss: 0.002046 | Epsilon: 0.0502\n",
      "Episode 5650 | Win Rate: 0.54 | Avg Loss: 0.001980 | Epsilon: 0.0502\n",
      "Episode 5700 | Win Rate: 0.45 | Avg Loss: 0.001734 | Epsilon: 0.0501\n",
      "Episode 5750 | Win Rate: 0.45 | Avg Loss: 0.001851 | Epsilon: 0.0500\n",
      "Episode 5800 | Win Rate: 0.51 | Avg Loss: 0.001998 | Epsilon: 0.0500\n",
      "Episode 5850 | Win Rate: 0.54 | Avg Loss: 0.001916 | Epsilon: 0.0500\n",
      "Episode 5900 | Win Rate: 0.54 | Avg Loss: 0.001859 | Epsilon: 0.0500\n",
      "Episode 5950 | Win Rate: 0.61 | Avg Loss: 0.001936 | Epsilon: 0.0500\n",
      "Episode 6000 | Win Rate: 0.62 | Avg Loss: 0.002067 | Epsilon: 0.0500\n",
      "Episode 6050 | Win Rate: 0.53 | Avg Loss: 0.002011 | Epsilon: 0.0500\n",
      "Episode 6100 | Win Rate: 0.52 | Avg Loss: 0.002016 | Epsilon: 0.0500\n",
      "Episode 6150 | Win Rate: 0.55 | Avg Loss: 0.002093 | Epsilon: 0.0500\n",
      "Episode 6200 | Win Rate: 0.51 | Avg Loss: 0.002134 | Epsilon: 0.0500\n",
      "Episode 6250 | Win Rate: 0.54 | Avg Loss: 0.002199 | Epsilon: 0.0500\n",
      "Episode 6300 | Win Rate: 0.52 | Avg Loss: 0.002028 | Epsilon: 0.0500\n",
      "Episode 6350 | Win Rate: 0.44 | Avg Loss: 0.001768 | Epsilon: 0.0500\n",
      "Episode 6400 | Win Rate: 0.56 | Avg Loss: 0.001831 | Epsilon: 0.0500\n",
      "Episode 6450 | Win Rate: 0.64 | Avg Loss: 0.002031 | Epsilon: 0.0500\n",
      "Episode 6500 | Win Rate: 0.58 | Avg Loss: 0.001984 | Epsilon: 0.0500\n",
      "Episode 6550 | Win Rate: 0.59 | Avg Loss: 0.001727 | Epsilon: 0.0500\n",
      "Episode 6600 | Win Rate: 0.68 | Avg Loss: 0.001678 | Epsilon: 0.0500\n",
      "Episode 6650 | Win Rate: 0.69 | Avg Loss: 0.001755 | Epsilon: 0.0500\n",
      "Episode 6700 | Win Rate: 0.62 | Avg Loss: 0.001737 | Epsilon: 0.0500\n",
      "Episode 6750 | Win Rate: 0.55 | Avg Loss: 0.001868 | Epsilon: 0.0500\n",
      "Episode 6800 | Win Rate: 0.54 | Avg Loss: 0.001992 | Epsilon: 0.0500\n",
      "Episode 6850 | Win Rate: 0.58 | Avg Loss: 0.001890 | Epsilon: 0.0500\n",
      "Episode 6900 | Win Rate: 0.60 | Avg Loss: 0.001870 | Epsilon: 0.0500\n",
      "Episode 6950 | Win Rate: 0.56 | Avg Loss: 0.001835 | Epsilon: 0.0500\n",
      "Episode 7000 | Win Rate: 0.54 | Avg Loss: 0.001788 | Epsilon: 0.0500\n",
      "Episode 7050 | Win Rate: 0.52 | Avg Loss: 0.001855 | Epsilon: 0.0500\n",
      "Episode 7100 | Win Rate: 0.51 | Avg Loss: 0.002173 | Epsilon: 0.0500\n",
      "Episode 7150 | Win Rate: 0.51 | Avg Loss: 0.002283 | Epsilon: 0.0500\n",
      "Episode 7200 | Win Rate: 0.53 | Avg Loss: 0.002161 | Epsilon: 0.0500\n",
      "Episode 7250 | Win Rate: 0.56 | Avg Loss: 0.002105 | Epsilon: 0.0500\n",
      "Episode 7300 | Win Rate: 0.52 | Avg Loss: 0.002072 | Epsilon: 0.0500\n",
      "Episode 7350 | Win Rate: 0.51 | Avg Loss: 0.001905 | Epsilon: 0.0500\n",
      "Episode 7400 | Win Rate: 0.57 | Avg Loss: 0.001625 | Epsilon: 0.0500\n",
      "Episode 7450 | Win Rate: 0.53 | Avg Loss: 0.001592 | Epsilon: 0.0500\n",
      "Episode 7500 | Win Rate: 0.42 | Avg Loss: 0.001712 | Epsilon: 0.0500\n",
      "Episode 7550 | Win Rate: 0.49 | Avg Loss: 0.001792 | Epsilon: 0.0500\n",
      "Episode 7600 | Win Rate: 0.64 | Avg Loss: 0.001624 | Epsilon: 0.0500\n",
      "Episode 7650 | Win Rate: 0.55 | Avg Loss: 0.001498 | Epsilon: 0.0500\n",
      "Episode 7700 | Win Rate: 0.52 | Avg Loss: 0.001502 | Epsilon: 0.0500\n",
      "Episode 7750 | Win Rate: 0.68 | Avg Loss: 0.001400 | Epsilon: 0.0500\n",
      "Episode 7800 | Win Rate: 0.58 | Avg Loss: 0.001421 | Epsilon: 0.0500\n",
      "Episode 7850 | Win Rate: 0.39 | Avg Loss: 0.001557 | Epsilon: 0.0500\n",
      "Episode 7900 | Win Rate: 0.45 | Avg Loss: 0.001853 | Epsilon: 0.0500\n",
      "Episode 7950 | Win Rate: 0.50 | Avg Loss: 0.001938 | Epsilon: 0.0500\n",
      "Episode 8000 | Win Rate: 0.53 | Avg Loss: 0.001842 | Epsilon: 0.0500\n",
      "Episode 8050 | Win Rate: 0.56 | Avg Loss: 0.001908 | Epsilon: 0.0500\n",
      "Episode 8100 | Win Rate: 0.54 | Avg Loss: 0.001574 | Epsilon: 0.0500\n",
      "Episode 8150 | Win Rate: 0.57 | Avg Loss: 0.001395 | Epsilon: 0.0500\n",
      "Episode 8200 | Win Rate: 0.57 | Avg Loss: 0.001478 | Epsilon: 0.0500\n",
      "Episode 8250 | Win Rate: 0.55 | Avg Loss: 0.001472 | Epsilon: 0.0500\n",
      "Episode 8300 | Win Rate: 0.56 | Avg Loss: 0.001645 | Epsilon: 0.0500\n",
      "Episode 8350 | Win Rate: 0.61 | Avg Loss: 0.001723 | Epsilon: 0.0500\n",
      "Episode 8400 | Win Rate: 0.64 | Avg Loss: 0.001566 | Epsilon: 0.0500\n",
      "Episode 8450 | Win Rate: 0.60 | Avg Loss: 0.001474 | Epsilon: 0.0500\n",
      "Episode 8500 | Win Rate: 0.60 | Avg Loss: 0.001541 | Epsilon: 0.0500\n",
      "Episode 8550 | Win Rate: 0.62 | Avg Loss: 0.001520 | Epsilon: 0.0500\n",
      "Episode 8600 | Win Rate: 0.51 | Avg Loss: 0.001591 | Epsilon: 0.0500\n",
      "Episode 8650 | Win Rate: 0.46 | Avg Loss: 0.001689 | Epsilon: 0.0500\n",
      "Episode 8700 | Win Rate: 0.43 | Avg Loss: 0.001618 | Epsilon: 0.0500\n",
      "Episode 8750 | Win Rate: 0.41 | Avg Loss: 0.001678 | Epsilon: 0.0500\n",
      "Episode 8800 | Win Rate: 0.52 | Avg Loss: 0.001958 | Epsilon: 0.0500\n",
      "Episode 8850 | Win Rate: 0.57 | Avg Loss: 0.002045 | Epsilon: 0.0500\n",
      "Episode 8900 | Win Rate: 0.56 | Avg Loss: 0.001845 | Epsilon: 0.0500\n",
      "Episode 8950 | Win Rate: 0.62 | Avg Loss: 0.001781 | Epsilon: 0.0500\n",
      "Episode 9000 | Win Rate: 0.63 | Avg Loss: 0.001697 | Epsilon: 0.0500\n",
      "Episode 9050 | Win Rate: 0.57 | Avg Loss: 0.001652 | Epsilon: 0.0500\n",
      "Episode 9100 | Win Rate: 0.62 | Avg Loss: 0.001809 | Epsilon: 0.0500\n",
      "Episode 9150 | Win Rate: 0.66 | Avg Loss: 0.001744 | Epsilon: 0.0500\n",
      "Episode 9200 | Win Rate: 0.59 | Avg Loss: 0.001724 | Epsilon: 0.0500\n",
      "Episode 9250 | Win Rate: 0.60 | Avg Loss: 0.001797 | Epsilon: 0.0500\n",
      "Episode 9300 | Win Rate: 0.55 | Avg Loss: 0.001678 | Epsilon: 0.0500\n",
      "Episode 9350 | Win Rate: 0.45 | Avg Loss: 0.001648 | Epsilon: 0.0500\n",
      "Episode 9400 | Win Rate: 0.54 | Avg Loss: 0.001611 | Epsilon: 0.0500\n",
      "Episode 9450 | Win Rate: 0.54 | Avg Loss: 0.001568 | Epsilon: 0.0500\n",
      "Episode 9500 | Win Rate: 0.50 | Avg Loss: 0.001811 | Epsilon: 0.0500\n",
      "Episode 9550 | Win Rate: 0.57 | Avg Loss: 0.002149 | Epsilon: 0.0500\n",
      "Episode 9600 | Win Rate: 0.60 | Avg Loss: 0.002190 | Epsilon: 0.0500\n",
      "Episode 9650 | Win Rate: 0.53 | Avg Loss: 0.002115 | Epsilon: 0.0500\n",
      "Episode 9700 | Win Rate: 0.47 | Avg Loss: 0.002149 | Epsilon: 0.0500\n",
      "Episode 9750 | Win Rate: 0.49 | Avg Loss: 0.002091 | Epsilon: 0.0500\n",
      "Episode 9800 | Win Rate: 0.47 | Avg Loss: 0.002256 | Epsilon: 0.0500\n",
      "Episode 9850 | Win Rate: 0.45 | Avg Loss: 0.002322 | Epsilon: 0.0500\n",
      "Episode 9900 | Win Rate: 0.42 | Avg Loss: 0.002235 | Epsilon: 0.0500\n",
      "Episode 9950 | Win Rate: 0.45 | Avg Loss: 0.002154 | Epsilon: 0.0500\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "# Initialize Environment and Agent\n",
    "environment = ConnectFourEnv()\n",
    "agent = DQNAgent(input_dim=42, output_dim=7)\n",
    "\n",
    "episodes = 10000\n",
    "update_freq = 10\n",
    "\n",
    "model = training(env=environment, agent=agent, num_episodes=episodes, target_update_freq=update_freq)\n",
    "\n",
    "# Save the policy network (the one that plays the game)\n",
    "torch.save(model.policy_net.state_dict(), \"connect4_dqn.pth\")\n",
    "print(\"Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
