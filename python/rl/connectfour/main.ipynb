{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e9ef0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ConnectFourEnv():\n",
    "    def __init__(self) -> None:\n",
    "        self.rows = 6\n",
    "        self.cols = 7\n",
    "        # 1D board of size 42\n",
    "        self.board = np.zeros(shape=(self.rows * self.cols,), dtype=int)\n",
    "        self.computer = 1\n",
    "        self.opponent = -1\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the board and handles who moves first.\"\"\"\n",
    "        self.board[:] = 0\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        \n",
    "        # Randomly decide who goes first\n",
    "        self.mover = np.random.choice([self.computer, self.opponent])\n",
    "        \n",
    "        # If opponent starts, they make a random move immediately\n",
    "        if self.mover == self.opponent:\n",
    "            action = self.random_action()\n",
    "            self.apply_action(action, self.opponent)\n",
    "            \n",
    "        return self.board.copy()\n",
    "\n",
    "    def available_actions_idx(self):\n",
    "        \"\"\"Returns a list of column indices (0-6) that are not full.\"\"\"\n",
    "        # Reshape to 2D to easily check the top row (row 0)\n",
    "        board_2d = self.board.reshape(self.rows, self.cols)\n",
    "        # If the top row (0) at column c is 0, the column is valid\n",
    "        return [c for c in range(self.cols) if board_2d[0, c] == 0]\n",
    "\n",
    "    def random_action(self):\n",
    "        \"\"\"Returns a random valid column.\"\"\"\n",
    "        possible_cols = self.available_actions_idx()\n",
    "        if not possible_cols:\n",
    "            return None # Draw/Full\n",
    "        return np.random.choice(possible_cols)\n",
    "\n",
    "    def apply_action(self, col_idx, player):\n",
    "        \"\"\"\n",
    "        Simulates gravity: places the player's piece in the \n",
    "        lowest available row in the given column.\n",
    "        \"\"\"\n",
    "        board_2d = self.board.reshape(self.rows, self.cols)\n",
    "        \n",
    "        # Find the lowest empty row in this column\n",
    "        # We scan from bottom (row 5) to top (row 0)\n",
    "        for r in range(self.rows - 1, -1, -1):\n",
    "            if board_2d[r, col_idx] == 0:\n",
    "                board_2d[r, col_idx] = player\n",
    "                break\n",
    "        \n",
    "        # Flatten back to 1D to update self.board\n",
    "        self.board = board_2d.flatten()\n",
    "\n",
    "    def check_win(self, player):\n",
    "        \"\"\"Checks horizontal, vertical, and diagonal lines for 4 connected.\"\"\"\n",
    "        board_2d = self.board.reshape(self.rows, self.cols)\n",
    "\n",
    "        # 1. Horizontal\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols - 3):\n",
    "                if np.all(board_2d[r, c:c+4] == player):\n",
    "                    return True\n",
    "\n",
    "        # 2. Vertical\n",
    "        for r in range(self.rows - 3):\n",
    "            for c in range(self.cols):\n",
    "                if np.all(board_2d[r:r+4, c] == player):\n",
    "                    return True\n",
    "\n",
    "        # 3. Diagonal (\\)\n",
    "        for r in range(self.rows - 3):\n",
    "            for c in range(self.cols - 3):\n",
    "                if np.all([board_2d[r+i, c+i] == player for i in range(4)]):\n",
    "                    return True\n",
    "\n",
    "        # 4. Anti-Diagonal (/)\n",
    "        for r in range(3, self.rows):\n",
    "            for c in range(self.cols - 3):\n",
    "                if np.all([board_2d[r-i, c+i] == player for i in range(4)]):\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def step(self, action, opponent_model=None): # <--- Check this argument\n",
    "        # 1. Check Agent Valid Move\n",
    "        if action not in self.available_actions_idx():\n",
    "             return self.board.copy(), -10, True, {\"result\": \"Error\"}\n",
    "        \n",
    "        # 2. Agent Move\n",
    "        self.apply_action(action, self.computer)\n",
    "        if self.check_win(self.computer):\n",
    "            return self.board.copy(), 1, True, {\"result\": \"Win\"}\n",
    "        if len(self.available_actions_idx()) == 0:\n",
    "            return self.board.copy(), 0, True, {\"result\": \"Draw\"}\n",
    "\n",
    "        # 3. Opponent Move\n",
    "        if opponent_model is None:\n",
    "            # Default: Random\n",
    "            opp_action = self.random_action()\n",
    "        else:\n",
    "            # Advanced: The Clone\n",
    "            opp_action = self.get_opponent_action(opponent_model) # <--- Make sure this is called\n",
    "            \n",
    "        self.apply_action(opp_action, self.opponent)\n",
    "\n",
    "        if self.check_win(self.opponent):\n",
    "            return self.board.copy(), -1, True, {\"result\": \"Loss\"}\n",
    "        if len(self.available_actions_idx()) == 0:\n",
    "            return self.board.copy(), 0, True, {\"result\": \"Draw\"}\n",
    "\n",
    "        return self.board.copy(), 0, False, {}\n",
    "\n",
    "    def get_opponent_action(self, model):\n",
    "        # 1. Prepare the board (Flip perspective)\n",
    "        board_for_opp = self.board * -1 \n",
    "        \n",
    "        # 2. Create the tensor (defaults to CPU)\n",
    "        state_t = torch.tensor(board_for_opp, dtype=torch.float32).unsqueeze(0).view(1, 1, 6, 7)\n",
    "        \n",
    "        # 3. CRITICAL FIX: Move tensor to the same device as the model (CPU or GPU)\n",
    "        # We check the device of the first parameter of the model\n",
    "        device = next(model.parameters()).device\n",
    "        state_t = state_t.to(device)\n",
    "        \n",
    "        # 4. Get the move\n",
    "        with torch.no_grad():\n",
    "            q_vals = model(state_t)\n",
    "            valid_moves = self.available_actions_idx()\n",
    "            \n",
    "            # Mask invalid moves\n",
    "            mask = torch.full_like(q_vals, -float('inf'))\n",
    "            mask[0, valid_moves] = q_vals[0, valid_moves]\n",
    "            \n",
    "            action = mask.max(1)[1].item()\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Visualizes the board.\"\"\"\n",
    "        board_2d = self.board.reshape(self.rows, self.cols)\n",
    "        symbols = {0: '.', 1: 'X', -1: 'O'}\n",
    "        print(\"\\nBoard State:\")\n",
    "        for row in board_2d:\n",
    "            print(\" \".join([symbols[x] for x in row]))\n",
    "        print(\"-\" * 13)\n",
    "        print(\"0 1 2 3 4 5 6\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e44fa562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNConnectFour(nn.Module):\n",
    "    def __init__(self, output_dim=7):\n",
    "        super(QNConnectFour, self).__init__()\n",
    "        \n",
    "        # --- Convolutional Block ---\n",
    "        # We treat the board as an image: 1 channel (the values -1, 0, 1), 6 rows, 7 cols\n",
    "        \n",
    "        # Conv1: Expands features. looks for small local patterns\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64) # Normalization helps faster convergence\n",
    "        \n",
    "        # Conv2: Goes deeper\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Conv3: Refines features\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # --- Fully Connected Block ---\n",
    "        # Flatten: 128 channels * 6 rows * 7 cols = 5376\n",
    "        self.fc1 = nn.Linear(128 * 6 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim) # Output is 7 (one Q-value per column)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Reshape Input\n",
    "        # The environment gives us a flat vector (Batch, 42).\n",
    "        # We must reshape it to (Batch, 1, 6, 7) for the CNN.\n",
    "        x = x.view(-1, 1, 6, 7) \n",
    "        \n",
    "        # 2. Convolutions + Activations\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # 3. Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # 4. Dense Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # 5. Output (No activation here, raw Q-values)\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f7fdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf42ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # 1. Initialize Networks\n",
    "        # Policy Net: The one we train\n",
    "        self.policy_net = QNConnectFour(output_dim).to(self.device)\n",
    "        # Target Net: A stable copy to calculate future rewards (stabilizes training)\n",
    "        self.target_net = QNConnectFour(output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval() # Set to evaluation mode\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.00001)\n",
    "        self.memory = ReplayMemory(10000)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.GAMMA = 0.99  # Discount factor (cares about long term)\n",
    "        self.EPS_START = 1.0\n",
    "        self.EPS_END = 0.005\n",
    "        self.EPS_DECAY = 1000 # How fast exploration decays\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, state, valid_moves):\n",
    "        \"\"\"\n",
    "        Epsilon-Greedy strategy with invalid move masking.\n",
    "        \"\"\"\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "\n",
    "        # EXPLORATION: Pick random valid move\n",
    "        if sample < eps_threshold:\n",
    "            return torch.tensor([[random.choice(valid_moves)]], device=self.device, dtype=torch.long)\n",
    "        \n",
    "        # EXPLOITATION: Pick best move from Network\n",
    "        with torch.no_grad():\n",
    "            # Get Q-values from network\n",
    "            q_values = self.policy_net(state.to(self.device))\n",
    "            \n",
    "            # Mask invalid moves: Set their Q-value to negative infinity so they aren't picked\n",
    "            # Create a mask of -inf\n",
    "            mask = torch.full_like(q_values, -float('inf'))\n",
    "            # Set valid indices to the actual q_values\n",
    "            mask[0, valid_moves] = q_values[0, valid_moves]\n",
    "            \n",
    "            # Return index of max value\n",
    "            return mask.max(1)[1].view(1, 1)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Convert batch data to tensors\n",
    "        state_batch = torch.cat(batch.state).to(self.device)\n",
    "        action_batch = torch.cat(batch.action).to(self.device)\n",
    "        reward_batch = torch.cat(batch.reward).to(self.device)\n",
    "        next_state_batch = torch.cat(batch.next_state).to(self.device)\n",
    "        done_batch = torch.cat(batch.done).to(self.device)\n",
    "\n",
    "        # 1. Compute Q(s_t, a) - The Q-values we estimated\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 2. Compute V(s_{t+1}) for all next states using Target Net\n",
    "        next_state_values = self.target_net(next_state_batch).max(1)[0].detach()\n",
    "        \n",
    "        # 3. Compute the expected Q values (Bellman Equation)\n",
    "        # If done, expected_q is just reward. If not, reward + gamma * best_future_q\n",
    "        expected_state_action_values = reward_batch + (self.GAMMA * next_state_values * (1 - done_batch))\n",
    "\n",
    "        # 4. Compute Huber Loss (Smooth L1)\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 5. Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients (common in RL)\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dfad320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import torch\n",
    "\n",
    "def training(env, agent, num_episodes=1000):\n",
    "\n",
    "    # Initialize\n",
    "    opponent_net = None \n",
    "    win_history = []\n",
    "    loss_history = [] # <--- NEW: To store average loss per episode\n",
    "    win_rate_threshold = 0.85\n",
    "\n",
    "    print(\"Starting Training...\")\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        state_np = env.reset()\n",
    "        state = torch.tensor(state_np, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # <--- NEW: Variables to track loss within this specific episode\n",
    "        episode_loss_sum = 0\n",
    "        episode_opt_count = 0 \n",
    "        \n",
    "        while not done:\n",
    "            # 1. Select Action\n",
    "            valid_moves = env.available_actions_idx()\n",
    "            action_tensor = agent.select_action(state, valid_moves)\n",
    "            action = action_tensor.item() \n",
    "            \n",
    "            # 2. Step Environment\n",
    "            next_state_np, reward, done, info = env.step(action, opponent_model=opponent_net)\n",
    "            \n",
    "            # 3. Process Reward & Next State\n",
    "            reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "            next_state = torch.tensor(next_state_np, dtype=torch.float32).unsqueeze(0)\n",
    "            done_tensor = torch.tensor([float(done)], dtype=torch.float32)\n",
    "\n",
    "            # 4. Store in Memory\n",
    "            agent.memory.push(state, action_tensor, reward_tensor, next_state, done_tensor)\n",
    "\n",
    "            # 5. Move to next state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # 6. Perform one step of optimization\n",
    "            loss = agent.optimize_model()\n",
    "            \n",
    "            # <--- NEW: Accumulate loss\n",
    "            if loss is not None:\n",
    "                episode_loss_sum += loss.item() # .item() is crucial to save memory!\n",
    "                episode_opt_count += 1\n",
    "\n",
    "        # --- TRACKING LOSS --- # <--- NEW\n",
    "        if episode_opt_count > 0:\n",
    "            avg_ep_loss = episode_loss_sum / episode_opt_count\n",
    "            loss_history.append(avg_ep_loss)\n",
    "        else:\n",
    "            loss_history.append(0)\n",
    "\n",
    "        # --- TRACKING WINS ---\n",
    "        if info['result'] == 'Win':\n",
    "            win_history.append(1)\n",
    "        else:\n",
    "            win_history.append(0)\n",
    "            \n",
    "        # Keep only last 100 games\n",
    "        if len(win_history) > 100: win_history.pop(0)\n",
    "        if len(loss_history) > 100: loss_history.pop(0) # Keep loss history same size\n",
    "            \n",
    "        # --- THE UPDATE CHECK ---\n",
    "        if i_episode % 50 == 0 and len(win_history) == 100:\n",
    "            win_rate = sum(win_history) / 100\n",
    "            # <--- NEW: Calculate average loss over the last 100 episodes\n",
    "            avg_loss_stat = sum(loss_history) / len(loss_history)\n",
    "            \n",
    "            # Calculate epsilon for display\n",
    "            curr_eps = agent.EPS_END + (agent.EPS_START - agent.EPS_END) * math.exp(-1. * agent.steps_done / agent.EPS_DECAY)\n",
    "            \n",
    "            print(f\"Episode {i_episode} | Win Rate: {win_rate:.2f} | Avg Loss: {avg_loss_stat:.6f} | Epsilon: {curr_eps:.4f}\")\n",
    "            \n",
    "            if win_rate > win_rate_threshold:\n",
    "                print(f\"ðŸš€ PROMOTION! Agent (Win Rate {win_rate:.2f}) is now the Opponent.\")\n",
    "                \n",
    "                opponent_net = copy.deepcopy(agent.policy_net)\n",
    "                opponent_net.eval()\n",
    "                \n",
    "                win_history = [] \n",
    "                loss_history = [] # Optional: Reset loss history too if you want fresh stats\n",
    "                \n",
    "                agent.steps_done = int(agent.EPS_DECAY * 2)\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f8f593a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize Environment and Agent\n",
    "# environment = ConnectFourEnv()\n",
    "# agent = DQNAgent(input_dim=42, output_dim=7)\n",
    "# \n",
    "# episodes = 1000\n",
    "# \n",
    "# model = training(env=environment, agent=agent, num_episodes=episodes)\n",
    "# \n",
    "# # Save the policy network (the one that plays the game)\n",
    "# torch.save(model.policy_net.state_dict(), \"connect4_dqn_v1.pth\")\n",
    "# print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3e0b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â import torch\n",
    "#Â \n",
    "#Â # 1. Initialize the Environment and a \"Fresh\" Agent\n",
    "#Â env = ConnectFourEnv()\n",
    "#Â agent = DQNAgent(input_dim=42, output_dim=7)\n",
    "#Â \n",
    "#Â # 2. Load the Saved Weights\n",
    "#Â model_path = \"connect4_dqn_v6.pth\"\n",
    "#Â print(f\"Loading model from {model_path}...\")\n",
    "#Â #Â state_dict = torch.load(model_path)\n",
    "#Â \n",
    "#Â # 3. Apply weights to Policy Net\n",
    "#Â agent.policy_net.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "#Â \n",
    "#Â \n",
    "#Â # 4. CRITICAL: Apply weights to Target Net as well\n",
    "#Â # (Otherwise, the target net is random, and training will be unstable for the first few episodes)\n",
    "#Â agent.target_net.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "#Â \n",
    "#Â # 5. CRITICAL: Adjust Exploration (Epsilon)\n",
    "#Â # Since we are resuming, we don't want 100% random moves. \n",
    "#Â # We want to start with low exploration (e.g., 10% or 5%).\n",
    "#Â # We reverse-engineer the steps_done variable to force epsilon to be low.\n",
    "#Â # Formula: steps = -ln((epsilon - end) / (start - end)) * decay\n",
    "#Â desired_epsilon = 0.05 \n",
    "#Â agent.steps_done = int(-math.log((desired_epsilon - agent.EPS_END) / \n",
    "#Â                        (agent.EPS_START - agent.EPS_END)) * agent.EPS_DECAY)\n",
    "#Â \n",
    "#Â print(f\"Agent initialized with Epsilon starting at: {desired_epsilon*100:.1f}%\")\n",
    "#Â \n",
    "#Â # 6. Resume Training\n",
    "#Â # Pass this pre-loaded agent into your training function\n",
    "#Â print(\"Resuming training...\")\n",
    "#Â model = training(env=env, agent=agent, num_episodes=10000, target_update_freq=10)\n",
    "#Â \n",
    "#Â # 7. Save the new version\n",
    "#Â torch.save(model.policy_net.state_dict(), \"connect4_dqn_v7.pth\")\n",
    "#Â print(\"New model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c90da",
   "metadata": {},
   "source": [
    "## **Enhanced Training loop to iterate over \"updates\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "961d0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "\n",
    "def training_by_objectives(env, agent, target_promotions=3, win_rate_threshold=0.85):\n",
    "\n",
    "    # --- CRITICAL CHANGE FOR RESUMING ---\n",
    "    # Start the opponent as a copy of the current smart agent.\n",
    "    # If we leave it None, the smart agent will beat the random opponent \n",
    "    # instantly and get a \"free\" promotion without learning anything.\n",
    "    opponent_net = copy.deepcopy(agent.policy_net)\n",
    "    opponent_net.eval()\n",
    "    print(\"Initial Opponent set to current Agent copy.\")\n",
    "\n",
    "    # Initialize tracking\n",
    "    win_history = []\n",
    "    loss_history = [] \n",
    "    \n",
    "    promotions_count = 0\n",
    "    \n",
    "    # We use itertools.count() to loop infinitely until we break manually\n",
    "    for i_episode in itertools.count(start=1):\n",
    "        \n",
    "        # Stop condition\n",
    "        if promotions_count >= target_promotions:\n",
    "            print(f\"\\nTarget of {target_promotions} promotions reached! Stopping.\")\n",
    "            break\n",
    "\n",
    "        state_np = env.reset()\n",
    "        state = torch.tensor(state_np, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        episode_loss_sum = 0\n",
    "        episode_opt_count = 0 \n",
    "        \n",
    "        while not done:\n",
    "            # 1. Select Action\n",
    "            valid_moves = env.available_actions_idx()\n",
    "            action_tensor = agent.select_action(state, valid_moves)\n",
    "            action = action_tensor.item() \n",
    "            \n",
    "            # 2. Step Environment\n",
    "            next_state_np, reward, done, info = env.step(action, opponent_model=opponent_net)\n",
    "            \n",
    "            # 3. Process Reward & Next State\n",
    "            reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "            next_state = torch.tensor(next_state_np, dtype=torch.float32).unsqueeze(0)\n",
    "            done_tensor = torch.tensor([float(done)], dtype=torch.float32)\n",
    "\n",
    "            # 4. Store in Memory\n",
    "            agent.memory.push(state, action_tensor, reward_tensor, next_state, done_tensor)\n",
    "\n",
    "            # 5. Move to next state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # 6. Optimize\n",
    "            loss = agent.optimize_model()\n",
    "            \n",
    "            if loss is not None:\n",
    "                episode_loss_sum += loss.item()\n",
    "                episode_opt_count += 1\n",
    "\n",
    "        # --- END OF EPISODE METRICS ---\n",
    "        if episode_opt_count > 0:\n",
    "            loss_history.append(episode_loss_sum / episode_opt_count)\n",
    "        else:\n",
    "            loss_history.append(0)\n",
    "\n",
    "        if info['result'] == 'Win':\n",
    "            win_history.append(1)\n",
    "        else:\n",
    "            win_history.append(0)\n",
    "            \n",
    "        if len(win_history) > 100: win_history.pop(0)\n",
    "        if len(loss_history) > 100: loss_history.pop(0)\n",
    "            \n",
    "        # --- CHECK FOR PROMOTION ---\n",
    "        # We check every 50 episodes, but only if we have a full history of 100 games\n",
    "        if i_episode % 50 == 0 and len(win_history) == 100:\n",
    "            win_rate = sum(win_history) / 100\n",
    "            avg_loss = sum(loss_history) / len(loss_history)\n",
    "            \n",
    "            # Calc epsilon for display\n",
    "            curr_eps = agent.EPS_END + (agent.EPS_START - agent.EPS_END) * math.exp(-1. * agent.steps_done / agent.EPS_DECAY)\n",
    "            \n",
    "            print(f\"Episode {i_episode} | Win Rate: {win_rate:.2f} | Loss: {avg_loss:.5f} | Eps: {curr_eps:.4f} | Promotions: {promotions_count}/{target_promotions}\")\n",
    "            \n",
    "            if win_rate > win_rate_threshold:\n",
    "                promotions_count += 1\n",
    "                print(f\"ðŸš€ PROMOTION [{promotions_count}/{target_promotions}]! Agent is now the Opponent.\")\n",
    "                \n",
    "                # Update Opponent\n",
    "                opponent_net = copy.deepcopy(agent.policy_net)\n",
    "                opponent_net.eval()\n",
    "                \n",
    "                # Reset Stats for the new \"level\"\n",
    "                win_history = [] \n",
    "                \n",
    "                # Optional: Reset exploration slightly to help adapt to new opponent?\n",
    "                # agent.steps_done = int(agent.EPS_DECAY * 3) \n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66d0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previous champion: connect4_dqn_v8.pth...\n",
      "Starting objective-based training (Target: 3 Promotions)...\n",
      "Initial Opponent set to current Agent copy.\n",
      "Episode 100 | Win Rate: 0.63 | Loss: 0.01550 | Eps: 0.0303 | Promotions: 0/3\n",
      "Episode 150 | Win Rate: 0.68 | Loss: 0.00910 | Eps: 0.0176 | Promotions: 0/3\n",
      "Episode 200 | Win Rate: 0.66 | Loss: 0.00547 | Eps: 0.0116 | Promotions: 0/3\n",
      "Episode 250 | Win Rate: 0.75 | Loss: 0.00336 | Eps: 0.0084 | Promotions: 0/3\n",
      "Episode 300 | Win Rate: 0.80 | Loss: 0.00217 | Eps: 0.0068 | Promotions: 0/3\n",
      "Episode 350 | Win Rate: 0.71 | Loss: 0.00150 | Eps: 0.0059 | Promotions: 0/3\n",
      "Episode 400 | Win Rate: 0.75 | Loss: 0.00100 | Eps: 0.0055 | Promotions: 0/3\n",
      "Episode 450 | Win Rate: 0.77 | Loss: 0.00058 | Eps: 0.0052 | Promotions: 0/3\n",
      "Episode 500 | Win Rate: 0.61 | Loss: 0.00111 | Eps: 0.0051 | Promotions: 0/3\n",
      "Episode 550 | Win Rate: 0.61 | Loss: 0.00164 | Eps: 0.0051 | Promotions: 0/3\n",
      "Episode 600 | Win Rate: 0.62 | Loss: 0.00149 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 650 | Win Rate: 0.62 | Loss: 0.00120 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 700 | Win Rate: 0.76 | Loss: 0.00083 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 750 | Win Rate: 0.65 | Loss: 0.00063 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 800 | Win Rate: 0.64 | Loss: 0.00054 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 850 | Win Rate: 0.74 | Loss: 0.00046 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 900 | Win Rate: 0.74 | Loss: 0.00044 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 950 | Win Rate: 0.77 | Loss: 0.00048 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1000 | Win Rate: 0.72 | Loss: 0.00049 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1050 | Win Rate: 0.73 | Loss: 0.00053 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1100 | Win Rate: 0.79 | Loss: 0.00056 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1150 | Win Rate: 0.74 | Loss: 0.00051 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1200 | Win Rate: 0.72 | Loss: 0.00061 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1250 | Win Rate: 0.75 | Loss: 0.00093 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1300 | Win Rate: 0.74 | Loss: 0.00102 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1350 | Win Rate: 0.61 | Loss: 0.00142 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1400 | Win Rate: 0.60 | Loss: 0.00189 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1450 | Win Rate: 0.72 | Loss: 0.00143 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1500 | Win Rate: 0.78 | Loss: 0.00097 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1550 | Win Rate: 0.77 | Loss: 0.00085 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1600 | Win Rate: 0.72 | Loss: 0.00073 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1650 | Win Rate: 0.73 | Loss: 0.00081 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1700 | Win Rate: 0.69 | Loss: 0.00084 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1750 | Win Rate: 0.66 | Loss: 0.00077 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1800 | Win Rate: 0.68 | Loss: 0.00099 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1850 | Win Rate: 0.68 | Loss: 0.00109 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1900 | Win Rate: 0.64 | Loss: 0.00090 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 1950 | Win Rate: 0.60 | Loss: 0.00107 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2000 | Win Rate: 0.57 | Loss: 0.00142 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2050 | Win Rate: 0.55 | Loss: 0.00149 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2100 | Win Rate: 0.55 | Loss: 0.00174 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2150 | Win Rate: 0.39 | Loss: 0.00218 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2200 | Win Rate: 0.46 | Loss: 0.00224 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2250 | Win Rate: 0.67 | Loss: 0.00174 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2300 | Win Rate: 0.70 | Loss: 0.00118 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2350 | Win Rate: 0.79 | Loss: 0.00087 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2400 | Win Rate: 0.81 | Loss: 0.00074 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2450 | Win Rate: 0.72 | Loss: 0.00079 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2500 | Win Rate: 0.51 | Loss: 0.00128 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2550 | Win Rate: 0.35 | Loss: 0.00176 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2600 | Win Rate: 0.41 | Loss: 0.00153 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2650 | Win Rate: 0.57 | Loss: 0.00140 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2700 | Win Rate: 0.50 | Loss: 0.00184 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2750 | Win Rate: 0.47 | Loss: 0.00188 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2800 | Win Rate: 0.67 | Loss: 0.00163 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2850 | Win Rate: 0.64 | Loss: 0.00171 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2900 | Win Rate: 0.54 | Loss: 0.00174 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 2950 | Win Rate: 0.63 | Loss: 0.00143 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3000 | Win Rate: 0.70 | Loss: 0.00148 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3050 | Win Rate: 0.63 | Loss: 0.00155 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3100 | Win Rate: 0.68 | Loss: 0.00120 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3150 | Win Rate: 0.75 | Loss: 0.00099 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3200 | Win Rate: 0.56 | Loss: 0.00124 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3250 | Win Rate: 0.44 | Loss: 0.00163 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3300 | Win Rate: 0.37 | Loss: 0.00160 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3350 | Win Rate: 0.31 | Loss: 0.00130 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3400 | Win Rate: 0.44 | Loss: 0.00118 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3450 | Win Rate: 0.69 | Loss: 0.00116 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3500 | Win Rate: 0.82 | Loss: 0.00100 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3550 | Win Rate: 0.82 | Loss: 0.00076 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3600 | Win Rate: 0.85 | Loss: 0.00058 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3650 | Win Rate: 0.84 | Loss: 0.00064 | Eps: 0.0050 | Promotions: 0/3\n",
      "Episode 3700 | Win Rate: 0.88 | Loss: 0.00060 | Eps: 0.0050 | Promotions: 0/3\n",
      "ðŸš€ PROMOTION [1/3]! Agent is now the Opponent.\n",
      "Episode 3800 | Win Rate: 0.74 | Loss: 0.00146 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 3850 | Win Rate: 0.79 | Loss: 0.00188 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 3900 | Win Rate: 0.80 | Loss: 0.00192 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 3950 | Win Rate: 0.52 | Loss: 0.00180 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4000 | Win Rate: 0.20 | Loss: 0.00171 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4050 | Win Rate: 0.18 | Loss: 0.00155 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4100 | Win Rate: 0.22 | Loss: 0.00140 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4150 | Win Rate: 0.33 | Loss: 0.00116 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4200 | Win Rate: 0.32 | Loss: 0.00116 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4250 | Win Rate: 0.47 | Loss: 0.00113 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4300 | Win Rate: 0.80 | Loss: 0.00091 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4350 | Win Rate: 0.80 | Loss: 0.00085 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4400 | Win Rate: 0.75 | Loss: 0.00078 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4450 | Win Rate: 0.76 | Loss: 0.00073 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4500 | Win Rate: 0.80 | Loss: 0.00068 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4550 | Win Rate: 0.83 | Loss: 0.00066 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4600 | Win Rate: 0.81 | Loss: 0.00058 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4650 | Win Rate: 0.79 | Loss: 0.00053 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4700 | Win Rate: 0.75 | Loss: 0.00059 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4750 | Win Rate: 0.72 | Loss: 0.00059 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4800 | Win Rate: 0.78 | Loss: 0.00062 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4850 | Win Rate: 0.76 | Loss: 0.00090 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4900 | Win Rate: 0.68 | Loss: 0.00129 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 4950 | Win Rate: 0.68 | Loss: 0.00123 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 5000 | Win Rate: 0.69 | Loss: 0.00093 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 5050 | Win Rate: 0.66 | Loss: 0.00082 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 5100 | Win Rate: 0.60 | Loss: 0.00089 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 5150 | Win Rate: 0.66 | Loss: 0.00076 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 5200 | Win Rate: 0.77 | Loss: 0.00047 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 5250 | Win Rate: 0.79 | Loss: 0.00053 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 5300 | Win Rate: 0.83 | Loss: 0.00070 | Eps: 0.0050 | Promotions: 1/3\n",
      "Episode 5350 | Win Rate: 0.87 | Loss: 0.00081 | Eps: 0.0050 | Promotions: 1/3\n",
      "ðŸš€ PROMOTION [2/3]! Agent is now the Opponent.\n",
      "Episode 5450 | Win Rate: 0.28 | Loss: 0.00180 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 5500 | Win Rate: 0.25 | Loss: 0.00177 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 5550 | Win Rate: 0.27 | Loss: 0.00181 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 5600 | Win Rate: 0.28 | Loss: 0.00192 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 5650 | Win Rate: 0.25 | Loss: 0.00189 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 5700 | Win Rate: 0.22 | Loss: 0.00199 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 5750 | Win Rate: 0.21 | Loss: 0.00190 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 5800 | Win Rate: 0.28 | Loss: 0.00177 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 5850 | Win Rate: 0.35 | Loss: 0.00195 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 5900 | Win Rate: 0.31 | Loss: 0.00190 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 5950 | Win Rate: 0.34 | Loss: 0.00168 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6000 | Win Rate: 0.39 | Loss: 0.00164 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6050 | Win Rate: 0.32 | Loss: 0.00175 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6100 | Win Rate: 0.37 | Loss: 0.00161 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6150 | Win Rate: 0.54 | Loss: 0.00157 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6200 | Win Rate: 0.73 | Loss: 0.00147 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6250 | Win Rate: 0.77 | Loss: 0.00113 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6300 | Win Rate: 0.74 | Loss: 0.00123 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6350 | Win Rate: 0.75 | Loss: 0.00143 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6400 | Win Rate: 0.54 | Loss: 0.00142 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6450 | Win Rate: 0.35 | Loss: 0.00138 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6500 | Win Rate: 0.28 | Loss: 0.00122 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6550 | Win Rate: 0.41 | Loss: 0.00119 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6600 | Win Rate: 0.69 | Loss: 0.00116 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6650 | Win Rate: 0.82 | Loss: 0.00089 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6700 | Win Rate: 0.85 | Loss: 0.00090 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6750 | Win Rate: 0.75 | Loss: 0.00105 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6800 | Win Rate: 0.73 | Loss: 0.00096 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6850 | Win Rate: 0.82 | Loss: 0.00078 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6900 | Win Rate: 0.81 | Loss: 0.00091 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 6950 | Win Rate: 0.79 | Loss: 0.00104 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 7000 | Win Rate: 0.78 | Loss: 0.00098 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 7050 | Win Rate: 0.74 | Loss: 0.00096 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 7100 | Win Rate: 0.79 | Loss: 0.00077 | Eps: 0.0050 | Promotions: 2/3\n",
      "Episode 7150 | Win Rate: 0.86 | Loss: 0.00072 | Eps: 0.0050 | Promotions: 2/3\n",
      "ðŸš€ PROMOTION [3/3]! Agent is now the Opponent.\n",
      "\n",
      "Target of 3 promotions reached! Stopping.\n",
      "Training complete. Saved as connect4_dqn_v8.pth\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup\n",
    "env = ConnectFourEnv()\n",
    "agent = DQNAgent(input_dim=42, output_dim=7)\n",
    "\n",
    "# 2. Load the v7 Model\n",
    "model_path = \"connect4_dqn_v8.pth\"\n",
    "print(f\"Loading previous champion: {model_path}...\")\n",
    "\n",
    "# Load weights (ensure you are on the right device)\n",
    "loaded_state = torch.load(model_path, map_location=\"cpu\")\n",
    "agent.policy_net.load_state_dict(loaded_state)\n",
    "agent.target_net.load_state_dict(loaded_state) # Don't forget target net!\n",
    "\n",
    "# 3. Adjust Epsilon\n",
    "# We want low exploration since the model is already smart,\n",
    "# but enough to find new strategies against the updated opponent.\n",
    "desired_epsilon = 0.1\n",
    "agent.steps_done = int(-math.log((desired_epsilon - agent.EPS_END) / \n",
    "                       (agent.EPS_START - agent.EPS_END)) * agent.EPS_DECAY)\n",
    "\n",
    "# 4. Run the Objective-Based Training\n",
    "print(\"Starting objective-based training (Target: 3 Promotions)...\")\n",
    "\n",
    "# Note: We don't pass num_episodes, only the target promotions\n",
    "model = training_by_objectives(\n",
    "    env=env, \n",
    "    agent=agent, \n",
    "    target_promotions=3, \n",
    "    win_rate_threshold=0.85 # High threshold ensures it really mastered the current opponent\n",
    ")\n",
    "\n",
    "# 5. Save v8\n",
    "torch.save(model.policy_net.state_dict(), \"connect4_dqn_v9.pth\")\n",
    "print(\"Training complete. Saved as connect4_dqn_v9.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
