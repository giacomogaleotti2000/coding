{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e9ef0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ConnectFourEnv():\n",
    "    def __init__(self) -> None:\n",
    "        self.rows = 6\n",
    "        self.cols = 7\n",
    "        # 1D board of size 42\n",
    "        self.board = np.zeros(shape=(self.rows * self.cols,), dtype=int)\n",
    "        self.computer = 1\n",
    "        self.opponent = -1\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the board and handles who moves first.\"\"\"\n",
    "        self.board[:] = 0\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        \n",
    "        # Randomly decide who goes first\n",
    "        self.mover = np.random.choice([self.computer, self.opponent])\n",
    "        \n",
    "        # If opponent starts, they make a random move immediately\n",
    "        if self.mover == self.opponent:\n",
    "            action = self.random_action()\n",
    "            self.apply_action(action, self.opponent)\n",
    "            \n",
    "        return self.board.copy()\n",
    "\n",
    "    def available_actions_idx(self):\n",
    "        \"\"\"Returns a list of column indices (0-6) that are not full.\"\"\"\n",
    "        # Reshape to 2D to easily check the top row (row 0)\n",
    "        board_2d = self.board.reshape(self.rows, self.cols)\n",
    "        # If the top row (0) at column c is 0, the column is valid\n",
    "        return [c for c in range(self.cols) if board_2d[0, c] == 0]\n",
    "\n",
    "    def random_action(self):\n",
    "        \"\"\"Returns a random valid column.\"\"\"\n",
    "        possible_cols = self.available_actions_idx()\n",
    "        if not possible_cols:\n",
    "            return None # Draw/Full\n",
    "        return np.random.choice(possible_cols)\n",
    "\n",
    "    def apply_action(self, col_idx, player):\n",
    "        \"\"\"\n",
    "        Simulates gravity: places the player's piece in the \n",
    "        lowest available row in the given column.\n",
    "        \"\"\"\n",
    "        board_2d = self.board.reshape(self.rows, self.cols)\n",
    "        \n",
    "        # Find the lowest empty row in this column\n",
    "        # We scan from bottom (row 5) to top (row 0)\n",
    "        for r in range(self.rows - 1, -1, -1):\n",
    "            if board_2d[r, col_idx] == 0:\n",
    "                board_2d[r, col_idx] = player\n",
    "                break\n",
    "        \n",
    "        # Flatten back to 1D to update self.board\n",
    "        self.board = board_2d.flatten()\n",
    "\n",
    "    def check_win(self, player):\n",
    "        \"\"\"Checks horizontal, vertical, and diagonal lines for 4 connected.\"\"\"\n",
    "        board_2d = self.board.reshape(self.rows, self.cols)\n",
    "\n",
    "        # 1. Horizontal\n",
    "        for r in range(self.rows):\n",
    "            for c in range(self.cols - 3):\n",
    "                if np.all(board_2d[r, c:c+4] == player):\n",
    "                    return True\n",
    "\n",
    "        # 2. Vertical\n",
    "        for r in range(self.rows - 3):\n",
    "            for c in range(self.cols):\n",
    "                if np.all(board_2d[r:r+4, c] == player):\n",
    "                    return True\n",
    "\n",
    "        # 3. Diagonal (\\)\n",
    "        for r in range(self.rows - 3):\n",
    "            for c in range(self.cols - 3):\n",
    "                if np.all([board_2d[r+i, c+i] == player for i in range(4)]):\n",
    "                    return True\n",
    "\n",
    "        # 4. Anti-Diagonal (/)\n",
    "        for r in range(3, self.rows):\n",
    "            for c in range(self.cols - 3):\n",
    "                if np.all([board_2d[r-i, c+i] == player for i in range(4)]):\n",
    "                    return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def step(self, action, opponent_model=None): # <--- Check this argument\n",
    "        # 1. Check Agent Valid Move\n",
    "        if action not in self.available_actions_idx():\n",
    "             return self.board.copy(), -10, True, {\"result\": \"Error\"}\n",
    "        \n",
    "        # 2. Agent Move\n",
    "        self.apply_action(action, self.computer)\n",
    "        if self.check_win(self.computer):\n",
    "            return self.board.copy(), 1, True, {\"result\": \"Win\"}\n",
    "        if len(self.available_actions_idx()) == 0:\n",
    "            return self.board.copy(), 0, True, {\"result\": \"Draw\"}\n",
    "\n",
    "        # 3. Opponent Move\n",
    "        if opponent_model is None:\n",
    "            # Default: Random\n",
    "            opp_action = self.random_action()\n",
    "        else:\n",
    "            # Advanced: The Clone\n",
    "            opp_action = self.get_opponent_action(opponent_model) # <--- Make sure this is called\n",
    "            \n",
    "        self.apply_action(opp_action, self.opponent)\n",
    "\n",
    "        if self.check_win(self.opponent):\n",
    "            return self.board.copy(), -1, True, {\"result\": \"Loss\"}\n",
    "        if len(self.available_actions_idx()) == 0:\n",
    "            return self.board.copy(), 0, True, {\"result\": \"Draw\"}\n",
    "\n",
    "        return self.board.copy(), 0, False, {}\n",
    "\n",
    "    def get_opponent_action(self, model):\n",
    "        # 1. Prepare the board (Flip perspective)\n",
    "        board_for_opp = self.board * -1 \n",
    "        \n",
    "        # 2. Create the tensor (defaults to CPU)\n",
    "        state_t = torch.tensor(board_for_opp, dtype=torch.float32).unsqueeze(0).view(1, 1, 6, 7)\n",
    "        \n",
    "        # 3. CRITICAL FIX: Move tensor to the same device as the model (CPU or GPU)\n",
    "        # We check the device of the first parameter of the model\n",
    "        device = next(model.parameters()).device\n",
    "        state_t = state_t.to(device)\n",
    "        \n",
    "        # 4. Get the move\n",
    "        with torch.no_grad():\n",
    "            q_vals = model(state_t)\n",
    "            valid_moves = self.available_actions_idx()\n",
    "            \n",
    "            # Mask invalid moves\n",
    "            mask = torch.full_like(q_vals, -float('inf'))\n",
    "            mask[0, valid_moves] = q_vals[0, valid_moves]\n",
    "            \n",
    "            action = mask.max(1)[1].item()\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Visualizes the board.\"\"\"\n",
    "        board_2d = self.board.reshape(self.rows, self.cols)\n",
    "        symbols = {0: '.', 1: 'X', -1: 'O'}\n",
    "        print(\"\\nBoard State:\")\n",
    "        for row in board_2d:\n",
    "            print(\" \".join([symbols[x] for x in row]))\n",
    "        print(\"-\" * 13)\n",
    "        print(\"0 1 2 3 4 5 6\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e44fa562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNConnectFour(nn.Module):\n",
    "    def __init__(self, output_dim=7):\n",
    "        super(QNConnectFour, self).__init__()\n",
    "        \n",
    "        # --- Convolutional Block ---\n",
    "        # We treat the board as an image: 1 channel (the values -1, 0, 1), 6 rows, 7 cols\n",
    "        \n",
    "        # Conv1: Expands features. looks for small local patterns\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64) # Normalization helps faster convergence\n",
    "        \n",
    "        # Conv2: Goes deeper\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Conv3: Refines features\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # --- Fully Connected Block ---\n",
    "        # Flatten: 128 channels * 6 rows * 7 cols = 5376\n",
    "        self.fc1 = nn.Linear(128 * 6 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim) # Output is 7 (one Q-value per column)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Reshape Input\n",
    "        # The environment gives us a flat vector (Batch, 42).\n",
    "        # We must reshape it to (Batch, 1, 6, 7) for the CNN.\n",
    "        x = x.view(-1, 1, 6, 7) \n",
    "        \n",
    "        # 2. Convolutions + Activations\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        # 3. Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # 4. Dense Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        # 5. Output (No activation here, raw Q-values)\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f7fdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf42ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.output_dim = output_dim\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # 1. Initialize Networks\n",
    "        # Policy Net: The one we train\n",
    "        self.policy_net = QNConnectFour(output_dim).to(self.device)\n",
    "        # Target Net: A stable copy to calculate future rewards (stabilizes training)\n",
    "        self.target_net = QNConnectFour(output_dim).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval() # Set to evaluation mode\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.00001)\n",
    "        self.memory = ReplayMemory(10000)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.GAMMA = 0.99  # Discount factor (cares about long term)\n",
    "        self.EPS_START = 1.0\n",
    "        self.EPS_END = 0.01\n",
    "        self.EPS_DECAY = 1000 # How fast exploration decays\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def select_action(self, state, valid_moves):\n",
    "        \"\"\"\n",
    "        Epsilon-Greedy strategy with invalid move masking.\n",
    "        \"\"\"\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "\n",
    "        # EXPLORATION: Pick random valid move\n",
    "        if sample < eps_threshold:\n",
    "            return torch.tensor([[random.choice(valid_moves)]], device=self.device, dtype=torch.long)\n",
    "        \n",
    "        # EXPLOITATION: Pick best move from Network\n",
    "        with torch.no_grad():\n",
    "            # Get Q-values from network\n",
    "            q_values = self.policy_net(state.to(self.device))\n",
    "            \n",
    "            # Mask invalid moves: Set their Q-value to negative infinity so they aren't picked\n",
    "            # Create a mask of -inf\n",
    "            mask = torch.full_like(q_values, -float('inf'))\n",
    "            # Set valid indices to the actual q_values\n",
    "            mask[0, valid_moves] = q_values[0, valid_moves]\n",
    "            \n",
    "            # Return index of max value\n",
    "            return mask.max(1)[1].view(1, 1)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Convert batch data to tensors\n",
    "        state_batch = torch.cat(batch.state).to(self.device)\n",
    "        action_batch = torch.cat(batch.action).to(self.device)\n",
    "        reward_batch = torch.cat(batch.reward).to(self.device)\n",
    "        next_state_batch = torch.cat(batch.next_state).to(self.device)\n",
    "        done_batch = torch.cat(batch.done).to(self.device)\n",
    "\n",
    "        # 1. Compute Q(s_t, a) - The Q-values we estimated\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 2. Compute V(s_{t+1}) for all next states using Target Net\n",
    "        next_state_values = self.target_net(next_state_batch).max(1)[0].detach()\n",
    "        \n",
    "        # 3. Compute the expected Q values (Bellman Equation)\n",
    "        # If done, expected_q is just reward. If not, reward + gamma * best_future_q\n",
    "        expected_state_action_values = reward_batch + (self.GAMMA * next_state_values * (1 - done_batch))\n",
    "\n",
    "        # 4. Compute Huber Loss (Smooth L1)\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 5. Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients (common in RL)\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfad320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import torch\n",
    "\n",
    "def training(env, agent, num_episodes=1000, target_update_freq=10):\n",
    "\n",
    "    # Initialize\n",
    "    opponent_net = None \n",
    "    win_history = []\n",
    "    loss_history = [] # <--- NEW: To store average loss per episode\n",
    "    win_rate_threshold = 0.85\n",
    "\n",
    "    print(\"Starting Training...\")\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        state_np = env.reset()\n",
    "        state = torch.tensor(state_np, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # <--- NEW: Variables to track loss within this specific episode\n",
    "        episode_loss_sum = 0\n",
    "        episode_opt_count = 0 \n",
    "        \n",
    "        while not done:\n",
    "            # 1. Select Action\n",
    "            valid_moves = env.available_actions_idx()\n",
    "            action_tensor = agent.select_action(state, valid_moves)\n",
    "            action = action_tensor.item() \n",
    "            \n",
    "            # 2. Step Environment\n",
    "            next_state_np, reward, done, info = env.step(action, opponent_model=opponent_net)\n",
    "            \n",
    "            # 3. Process Reward & Next State\n",
    "            reward_tensor = torch.tensor([reward], dtype=torch.float32)\n",
    "            next_state = torch.tensor(next_state_np, dtype=torch.float32).unsqueeze(0)\n",
    "            done_tensor = torch.tensor([float(done)], dtype=torch.float32)\n",
    "\n",
    "            # 4. Store in Memory\n",
    "            agent.memory.push(state, action_tensor, reward_tensor, next_state, done_tensor)\n",
    "\n",
    "            # 5. Move to next state\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # 6. Perform one step of optimization\n",
    "            loss = agent.optimize_model()\n",
    "            \n",
    "            # <--- NEW: Accumulate loss\n",
    "            if loss is not None:\n",
    "                episode_loss_sum += loss.item() # .item() is crucial to save memory!\n",
    "                episode_opt_count += 1\n",
    "\n",
    "        # --- TRACKING LOSS --- # <--- NEW\n",
    "        if episode_opt_count > 0:\n",
    "            avg_ep_loss = episode_loss_sum / episode_opt_count\n",
    "            loss_history.append(avg_ep_loss)\n",
    "        else:\n",
    "            loss_history.append(0)\n",
    "\n",
    "        # --- TRACKING WINS ---\n",
    "        if info['result'] == 'Win':\n",
    "            win_history.append(1)\n",
    "        else:\n",
    "            win_history.append(0)\n",
    "            \n",
    "        # Keep only last 100 games\n",
    "        if len(win_history) > 100: win_history.pop(0)\n",
    "        if len(loss_history) > 100: loss_history.pop(0) # Keep loss history same size\n",
    "            \n",
    "        # --- THE UPDATE CHECK ---\n",
    "        if i_episode % 50 == 0 and len(win_history) == 100:\n",
    "            win_rate = sum(win_history) / 100\n",
    "            # <--- NEW: Calculate average loss over the last 100 episodes\n",
    "            avg_loss_stat = sum(loss_history) / len(loss_history)\n",
    "            \n",
    "            # Calculate epsilon for display\n",
    "            curr_eps = agent.EPS_END + (agent.EPS_START - agent.EPS_END) * math.exp(-1. * agent.steps_done / agent.EPS_DECAY)\n",
    "            \n",
    "            print(f\"Episode {i_episode} | Win Rate: {win_rate:.2f} | Avg Loss: {avg_loss_stat:.6f} | Epsilon: {curr_eps:.4f}\")\n",
    "            \n",
    "            if win_rate > win_rate_threshold:\n",
    "                print(f\"ðŸš€ PROMOTION! Agent (Win Rate {win_rate:.2f}) is now the Opponent.\")\n",
    "                \n",
    "                opponent_net = copy.deepcopy(agent.policy_net)\n",
    "                opponent_net.eval()\n",
    "                \n",
    "                win_history = [] \n",
    "                loss_history = [] # Optional: Reset loss history too if you want fresh stats\n",
    "                \n",
    "                agent.steps_done = int(agent.EPS_DECAY * 2)\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8f593a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize Environment and Agent\n",
    "# environment = ConnectFourEnv()\n",
    "# agent = DQNAgent(input_dim=42, output_dim=7)\n",
    "# \n",
    "# episodes = 1000\n",
    "# update_freq = 10\n",
    "# \n",
    "# model = training(env=environment, agent=agent, num_episodes=episodes, target_update_freq=update_freq)\n",
    "# \n",
    "# # Save the policy network (the one that plays the game)\n",
    "# torch.save(model.policy_net.state_dict(), \"connect4_dqn_v1.pth\")\n",
    "# print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e0b074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from connect4_dqn_v6.pth...\n",
      "Agent initialized with Epsilon starting at: 5.0%\n",
      "Resuming training...\n",
      "Starting Training...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Initialize the Environment and a \"Fresh\" Agent\n",
    "env = ConnectFourEnv()\n",
    "agent = DQNAgent(input_dim=42, output_dim=7)\n",
    "\n",
    "# 2. Load the Saved Weights\n",
    "model_path = \"connect4_dqn_v6.pth\"\n",
    "print(f\"Loading model from {model_path}...\")\n",
    "#Â state_dict = torch.load(model_path)\n",
    "\n",
    "# 3. Apply weights to Policy Net\n",
    "agent.policy_net.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "\n",
    "\n",
    "# 4. CRITICAL: Apply weights to Target Net as well\n",
    "# (Otherwise, the target net is random, and training will be unstable for the first few episodes)\n",
    "agent.target_net.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "\n",
    "# 5. CRITICAL: Adjust Exploration (Epsilon)\n",
    "# Since we are resuming, we don't want 100% random moves. \n",
    "# We want to start with low exploration (e.g., 10% or 5%).\n",
    "# We reverse-engineer the steps_done variable to force epsilon to be low.\n",
    "# Formula: steps = -ln((epsilon - end) / (start - end)) * decay\n",
    "desired_epsilon = 0.05 \n",
    "agent.steps_done = int(-math.log((desired_epsilon - agent.EPS_END) / \n",
    "                       (agent.EPS_START - agent.EPS_END)) * agent.EPS_DECAY)\n",
    "\n",
    "print(f\"Agent initialized with Epsilon starting at: {desired_epsilon*100:.1f}%\")\n",
    "\n",
    "# 6. Resume Training\n",
    "# Pass this pre-loaded agent into your training function\n",
    "print(\"Resuming training...\")\n",
    "model = training(env=env, agent=agent, num_episodes=10000, target_update_freq=10)\n",
    "\n",
    "# 7. Save the new version\n",
    "torch.save(model.policy_net.state_dict(), \"connect4_dqn_v7.pth\")\n",
    "print(\"New model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
